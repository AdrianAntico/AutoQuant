---
title: "MultiClass Evaluation & Insights"
author: "Provided by RemixAutoML"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  prettydoc::html_pretty:
    theme: remixautoml
    toc: yes
    toc_depth: 2
    fig_caption: yes
    number_sections: yes
classoption: landscape
---

```{r Environment, include=FALSE}
# theme options up top: hpstr, cayman, architect, remixautoml
knitr::opts_chunk$set(echo = TRUE)
temp <- globalenv()
TempNames <- names(temp)
for(nam in TempNames) {
  assign(x = nam, value = eval(temp[[nam]]), envir = .GlobalEnv)
}
```



```{r RemixOutput_Model_MetaData, echo = FALSE}
if(!is.null(RemixOutput)) {
  
  # Model MetaData ----

  ## Model_MetaData_Parameters ----
  ArgsList <- RemixOutput[['ArgsList']]
  
  ## Model_MetaData_GridMetrics ----
  GridMetrics <- RemixOutput[['GridMetrics']]
  
}
``` 

```{r RemixOutput_DataSets_And_MetaData, echo = FALSE}
if(!is.null(RemixOutput)) {
  
  # DataSets
  TestData <- RemixOutput[['TestData']]
  ValidationData <- RemixOutput[['ValidationData']]
  TrainData <- RemixOutput[['TrainData']]
  
  # Meta info
  TargetColumnName <- RemixOutput[['ArgsList']][['TargetColumnName']]
  TargetLevels <- RemixOutput[['ArgsList']][['TargetLevels']]
  PredictionColumnName <- PredictionColumnName
  if(is.null(FeatureColumnNames)) {
    FeatureColumnNames <- RemixOutput[['ColNames']][[1L]]
  }
  if(is.null(DateColumnName) && !is.null(RemixOutput[['ArgsList']][['PrimaryDateColumn']])) {
    DateColumnName <- RemixOutput[['ArgsList']][['PrimaryDateColumn']]
  } else {
    DateColumnName <- NULL
  }
}
```

```{r RemixOutput_Evaluation_Metrics, echo = FALSE}
if(!is.null(RemixOutput)) {
  
  # Evaluation Metrics ----
  
  ## MultiClass Metrics ----
  Test_MultiClassMetrics <- RemixOutput[['MultinomialMetrics']][['TestData']]
  Train_MultiClassMetrics <- RemixOutput[['MultinomialMetrics']][['TrainData']]

  # Update Colnames
  if(!is.null(Test_MultiClassMetrics)) Test_MultiClassMetrics[, Data_Source := 'Test']
  if(!is.null(Train_MultiClassMetrics)) Train_MultiClassMetrics[, Data_Source := 'Train']

  # CatBoost only
  if(is.null(Test_MultiClassMetrics) && is.null(Train_MultiClassMetrics)) {
    All_MultiClassMetrics <- NULL
  } else if(!is.null(Test_MultiClassMetrics) && !is.null(Train_MultiClassMetrics)) {
    All_MultiClassMetrics <- data.table::rbindlist(list(
      Test_MultiClassMetrics, Train_MultiClassMetrics))
  } else if(is.null(Test_MultiClassMetrics) && !is.null(Train_MultiClassMetrics)) {
    All_MultiClassMetrics <- Train_MultiClassMetrics
  } else if(!is.null(Test_MultiClassMetrics) && is.null(Train_MultiClassMetrics)) {
    All_MultiClassMetrics <- Test_MultiClassMetrics
  } else {
    All_MultiClassMetrics <- NULL
  }
  
  
  ## Model_Evaluation_Metrics ----
  EvalMetricsNames <- names(RemixOutput[['EvaluationMetrics']])
  Test_EvalMetricss <- EvalMetricsNames[which(EvalMetricsNames %like% 'TestData_')]
  Test_EvalMetrics <- list()
  for(nam in Test_EvalMetricss) Test_EvalMetrics[[nam]] <- RemixOutput[['EvaluationMetrics']][[nam]]
  Train_EvalMetricss <- setdiff(EvalMetricsNames, Test_EvalMetricss)
  Train_EvalMetrics <- list()
  for(nam in Train_EvalMetricss) Train_EvalMetrics[[nam]] <- RemixOutput[['EvaluationMetrics']][[nam]]
  
  ## Model_VarImportanceTable ----
  if(tolower(Algo) == 'catboost') {
    Test_Importance <- RemixOutput[['VariableImportance']][['Test_Importance']]
    Validation_Importance <- RemixOutput[['VariableImportance']][['Validation_Importance']]
    Train_Importance <- RemixOutput[['VariableImportance']][['Train_Importance']]

    # Update Colnames
    if(!is.null(Test_Importance)) data.table::setnames(Test_Importance, old = 'Importance', new = 'Test_Importance', skip_absent = TRUE)
    if(!is.null(Validation_Importance)) data.table::setnames(Validation_Importance, old = 'Importance', new = 'Validation_Importance', skip_absent = TRUE)
    if(!is.null(Train_Importance)) data.table::setnames(Train_Importance, old = 'Importance', new = 'Train_Importance', skip_absent = TRUE)

    # CatBoost only
    if(is.null(Test_Importance) && is.null(Validation_Importance) && is.null(Train_Importance)) {
      All_Importance <- NULL
    } else if(!is.null(Test_Importance) && !is.null(Validation_Importance) && !is.null(Train_Importance)) {
      All_Importance <- merge(Test_Importance, Validation_Importance, by = 'Variable', all = TRUE)
      All_Importance <- merge(All_Importance, Train_Importance, by = 'Variable', all = TRUE)
    } else if(!is.null(Test_Importance) && !is.null(Validation_Importance) && is.null(Train_Importance)) {
      All_Importance <- merge(Test_Importance, Validation_Importance, by = 'Variable', all = TRUE)
    } else if(!is.null(Test_Importance) && is.null(Validation_Importance) && !is.null(Train_Importance)) {
      All_Importance <- merge(Test_Importance, Train_Importance, by = 'Variable', all = TRUE)
    } else if(is.null(Test_Importance) && !is.null(Validation_Importance) && !is.null(Train_Importance)) {
      All_Importance <- merge(Validation_Importance, Train_Importance, by = 'Variable', all = TRUE)
    } else if(is.null(Test_Importance) && is.null(Validation_Importance) && !is.null(Train_Importance)) {
      All_Importance <- Train_Importance
    } else if(is.null(Test_Importance) && !is.null(Validation_Importance) && is.null(Train_Importance)) {
      All_Importance <- Validation_Importance
    } else if(!is.null(Test_Importance) && is.null(Validation_Importance) && is.null(Train_Importance)) {
      All_Importance <- Test_Importance
    } else {
      All_Importance <- NULL
    }
    
  } else {
    
    #### Encoding-Based Models
    if(is.null(Test_Importance_dt)) {
      Test_Importance <- NULL
    } else {
      Test_Importance <- Test_Importance_dt
    }
    if(is.null(Validation_Importance_dt)) {
      Validation_Importance <- NULL
    } else {
      Validation_Importance <- Validation_Importance_dt
    }
    if(is.null(Train_Importance_dt)) {
      Train_Importance <- RemixOutput[['VariableImportance']]
    } else {
      Train_Importance <- Train_Importance_dt
    }
    
    # Update Colnames
    if(!is.null(Test_Importance)) data.table::setnames(Test_Importance, old = 'Importance', new = 'Test_Importance', skip_absent = TRUE)
    if(!is.null(Validation_Importance)) data.table::setnames(Validation_Importance, old = 'Importance', new = 'Validation_Importance', skip_absent = TRUE)
    if(!is.null(Train_Importance)) data.table::setnames(Train_Importance, old = 'Importance', new = 'Train_Importance', skip_absent = TRUE)

    # CatBoost only
    if(is.null(Test_Importance) && is.null(Validation_Importance) && is.null(Train_Importance)) {
      All_Importance <- NULL
    } else if(!is.null(Test_Importance) && !is.null(Validation_Importance) && !is.null(Train_Importance)) {
      All_Importance <- merge(Test_Importance, Validation_Importance, by = 'Variable', all = TRUE)
      All_Importance <- merge(All_Importance, Train_Importance, by = 'Variable', all = TRUE)
    } else if(!is.null(Test_Importance) && !is.null(Validation_Importance) && is.null(Train_Importance)) {
      All_Importance <- merge(Test_Importance, Validation_Importance, by = 'Variable', all = TRUE)
    } else if(!is.null(Test_Importance) && is.null(Validation_Importance) && !is.null(Train_Importance)) {
      All_Importance <- merge(Test_Importance, Train_Importance, by = 'Variable', all = TRUE)
    } else if(is.null(Test_Importance) && !is.null(Validation_Importance) && !is.null(Train_Importance)) {
      All_Importance <- merge(Validation_Importance, Train_Importance, by = 'Variable', all = TRUE)
    } else if(is.null(Test_Importance) && is.null(Validation_Importance) && !is.null(Train_Importance)) {
      All_Importance <- Train_Importance
    } else if(is.null(Test_Importance) && !is.null(Validation_Importance) && is.null(Train_Importance)) {
      All_Importance <- Validation_Importance
    } else if(!is.null(Test_Importance) && is.null(Validation_Importance) && is.null(Train_Importance)) {
      All_Importance <- Test_Importance
    } else {
      All_Importance <- NULL
    }
    
  }
  
  ## Model_IntImportanceTable ----
  if(tolower(Algo) == 'catboost') {
    Test_Interaction <- RemixOutput[['InteractionImportance']][['Test_Interaction']]
    Validation_Interaction <- RemixOutput[['InteractionImportance']][['Validation_Interaction']]
    Train_Interaction <- RemixOutput[['InteractionImportance']][['Train_Interaction']]
    
    # Update Colnames
    if(!is.null(Test_Interaction)) data.table::setnames(Test_Interaction, old = 'score', new = 'Test_Importance', skip_absent = TRUE)
    if(!is.null(Validation_Interaction)) data.table::setnames(Validation_Interaction, old = 'score', new = 'Validation_Importance', skip_absent = TRUE)
    if(!is.null(Train_Interaction)) data.table::setnames(Train_Interaction, old = 'score', new = 'Train_Importance', skip_absent = TRUE)

    # CatBoost only
    if(is.null(Test_Interaction) && is.null(Validation_Interaction) && is.null(Train_Interaction)) {
      All_Interaction <- NULL
    } else if(!is.null(Test_Interaction) && !is.null(Validation_Interaction) && !is.null(Train_Interaction)) {
      All_Interaction <- merge(Test_Interaction, Validation_Interaction, by = c('Features1','Features2'), all = TRUE)
      All_Interaction <- merge(All_Interaction, Train_Interaction, by = c('Features1','Features2'), all = TRUE)
      data.table::setorderv(x = All_Interaction, cols = names(All_Interaction)[3L], order = -1)
    } else if(!is.null(Test_Interaction) && !is.null(Validation_Interaction) && is.null(Train_Interaction)) {
      All_Interaction <- merge(Test_Interaction, Validation_Interaction, by = c('Features1','Features2'), all = TRUE)
      data.table::setorderv(x = All_Interaction, cols = names(All_Interaction)[3L], order = -1)
    } else if(!is.null(Test_Interaction) && is.null(Validation_Interaction) && !is.null(Train_Interaction)) {
      All_Interaction <- merge(Test_Interaction, Train_Interaction, by = c('Features1','Features2'), all = TRUE)
      data.table::setorderv(x = All_Interaction, cols = names(All_Interaction)[3L], order = -1)
    } else if(is.null(Test_Interaction) && !is.null(Validation_Interaction) && !is.null(Train_Interaction)) {
      All_Interaction <- merge(Validation_Interaction, Train_Interaction, by = c('Features1','Features2'), all = TRUE)
      data.table::setorderv(x = All_Interaction, cols = names(All_Interaction)[3L], order = -1)
    } else if(is.null(Test_Interaction) && is.null(Validation_Interaction) && !is.null(Train_Interaction)) {
      All_Interaction <- Train_Interaction
    } else if(is.null(Test_Interaction) && !is.null(Validation_Interaction) && is.null(Train_Interaction)) {
      All_Interaction <- Validation_Interaction
    } else if(!is.null(Test_Interaction) && is.null(Validation_Interaction) && is.null(Train_Interaction)) {
      All_Interaction <- Test_Interaction
    } else {
      All_Interaction <- NULL
    }
    
  } else {
    
    # Encoding-based Models + Generic Connector
    if(is.null(Test_Interaction_dt)) {
      Test_Interaction <- NULL
    } else {
      Test_Interaction <- Test_Interaction_dt
    }
    if(is.null(Validation_Interaction_dt)) {
      Validation_Interaction <- NULL
    } else {
      Validation_Interaction <- Validation_Interaction_dt
    }
    if(is.null(Train_Interaction_dt)) {
      Train_Interaction <- NULL
    } else {
      Train_Interaction <- Train_Interaction_dt
    }
    
    # Update Colnames
    if(!is.null(Test_Interaction)) data.table::setnames(Test_Interaction, old = 'score', new = 'Test_Importance', skip_absent = TRUE)
    if(!is.null(Validation_Interaction)) data.table::setnames(Validation_Interaction, old = 'score', new = 'Validation_Importance', skip_absent = TRUE)
    if(!is.null(Train_Interaction)) data.table::setnames(Train_Interaction, old = 'score', new = 'Train_Importance', skip_absent = TRUE)

    # CatBoost only
    if(is.null(Test_Interaction) && is.null(Validation_Interaction) && is.null(Train_Interaction)) {
      All_Interaction <- NULL
    } else if(!is.null(Test_Interaction) && !is.null(Validation_Interaction) && !is.null(Train_Interaction)) {
      All_Interaction <- merge(Test_Interaction, Validation_Interaction, by = c('Features1','Features2'), all = TRUE)
      All_Interaction <- merge(All_Interaction, Train_Interaction, by = c('Features1','Features2'), all = TRUE)
      data.table::setorderv(x = All_Interaction, cols = names(All_Interaction)[3L], order = -1)
    } else if(!is.null(Test_Interaction) && !is.null(Validation_Interaction) && is.null(Train_Interaction)) {
      All_Interaction <- merge(Test_Interaction, Validation_Interaction, by = c('Features1','Features2'), all = TRUE)
      data.table::setorderv(x = All_Interaction, cols = names(All_Interaction)[3L], order = -1)
    } else if(!is.null(Test_Interaction) && is.null(Validation_Interaction) && !is.null(Train_Interaction)) {
      All_Interaction <- merge(Test_Interaction, Train_Interaction, by = c('Features1','Features2'), all = TRUE)
      data.table::setorderv(x = All_Interaction, cols = names(All_Interaction)[3L], order = -1)
    } else if(is.null(Test_Interaction) && !is.null(Validation_Interaction) && !is.null(Train_Interaction)) {
      All_Interaction <- merge(Validation_Interaction, Train_Interaction, by = c('Features1','Features2'), all = TRUE)
      data.table::setorderv(x = All_Interaction, cols = names(All_Interaction)[3L], order = -1)
    } else if(is.null(Test_Interaction) && is.null(Validation_Interaction) && !is.null(Train_Interaction)) {
      All_Interaction <- Train_Interaction
    } else if(is.null(Test_Interaction) && !is.null(Validation_Interaction) && is.null(Train_Interaction)) {
      All_Interaction <- Validation_Interaction
    } else if(!is.null(Test_Interaction) && is.null(Validation_Interaction) && is.null(Train_Interaction)) {
      All_Interaction <- Test_Interaction
    } else {
      All_Interaction <- NULL
    }
    
  }
}
``` 

```{r RemixOutput_Evaluation_Plots, echo = FALSE}
if(!is.null(RemixOutput)) {

  # Evaluation Plots ----
  
  ## EvaluationPlots_CalibrationPlot ----
  EvalPlotNames <- names(RemixOutput[['PlotList']])
  Test_EvaluationPlotss <- EvalPlotNames[which(EvalPlotNames %like% 'Test_EvaluationPlot_')]
  Test_EvaluationPlot <- list()
  for(nam in Test_EvaluationPlotss) Test_EvaluationPlot[[nam]] <- RemixOutput[['PlotList']][[nam]]
  Train_EvaluationPlotss <- EvalPlotNames[which(EvalPlotNames %like% 'Train_EvaluationPlot_')]
  Train_EvaluationPlot <- list()
  for(nam in Train_EvaluationPlotss) Train_EvaluationPlot[[nam]] <- RemixOutput[['PlotList']][[nam]]
  
  ## ROCPlot ----
  EvalPlotNames <- names(RemixOutput[['PlotList']])
  Test_EvaluationPlotss <- EvalPlotNames[which(EvalPlotNames %like% 'Test_ROC_Plot_')]
  Test_ROCPlot <- list()
  for(nam in Test_EvaluationPlotss) Test_ROCPlot[[nam]] <- RemixOutput[['PlotList']][[nam]]
  Train_EvaluationPlotss <- EvalPlotNames[which(EvalPlotNames %like% 'Train_ROC_Plot_')]
  Train_ROCPlot <- list()
  for(nam in Train_EvaluationPlotss) Train_ROCPlot[[nam]] <- RemixOutput[['PlotList']][[nam]]
  
  ## Cumulative Gains Plot ----
  CumGainsNames <- names(RemixOutput[['PlotList']])
  Test_CumGainsCharts <- CumGainsNames[which(CumGainsNames %like% 'Test_GainsPlot_')]
  Test_CumGainsChart <- list()
  for(nam in Test_CumGainsCharts) Test_CumGainsChart[[nam]] <- RemixOutput[['PlotList']][[nam]]
  Train_CumGainsCharts <- CumGainsNames[which(CumGainsNames %like% 'Train_GainsPlot_')]
  Train_CumGainsChart <- list()
  for(nam in Train_CumGainsCharts) Train_CumGainsChart[[nam]] <- RemixOutput[['PlotList']][[nam]]
  
  # Lift Plots ----
  LiftPlotNames <- names(RemixOutput[['PlotList']])
  Test_LiftPlots <- LiftPlotNames[which(LiftPlotNames %like% 'Test_LiftPlot_')]
  Test_LiftPlot <- list()
  for(nam in Test_LiftPlots) Test_LiftPlot[[nam]] <- RemixOutput[['PlotList']][[nam]]
  Train_LiftPlots <- LiftPlotNames[which(LiftPlotNames %like% 'Train_LiftPlot_')]
  Train_LiftPlot <- list()
  for(nam in Train_LiftPlots) Train_LiftPlot[[nam]] <- RemixOutput[['PlotList']][[nam]]
}
```  

```{r RemixOutput_Model_Interpretation, echo = FALSE}
if(!is.null(RemixOutput)) {
  
  # Model Interpretation ----
  
  ## Model_Evaluation_Metrics_NumericVariables ----
  
  ### TestData ----
  
  # Plots to Add and Remove
  
  # Extra list mgt + par dep plot for multinomial
  
  # Numeric-Test: Starting batch of plots
  Test_ParDepPlots <- list()
  ParDepPlotsNames <- names(RemixOutput[['PlotList']])
  Test_ParDepPlotss <- gsub(pattern = 'Test_ParDepPlots_', replacement = '', x = ParDepPlotsNames[ParDepPlotsNames %like% 'Test_ParDepPlots_'])
  for(nam in Test_ParDepPlotss) {
    
    # Name to keep and remove
    Test_ParDepPlots[[nam]] <- RemixOutput$PlotList[[paste0('Test_ParDepPlots_', nam)]]

    # Remove Names
    A <- names(Test_ParDepPlots)
    B <- FeatureColumnNames
    RemovePDPList_Line <- setdiff(A, B)
    if(identical(character(0), RemovePDPList_Line)) RemovePDPList_Line <- NULL
    
    # Add Names
    AddPDPList_Line = setdiff(B, A)
    if(identical(character(0), AddPDPList_Line)) AddPDPList_Line <- NULL
    
    # Remove Plots from List per User Request
    if(!is.null(A)) {
      
      # Loop through all names of PDP List
      for(v in A) {
  
        # Remove Plots
        if(!is.null(TestData) && (!is.numeric(TestData[[v]]) || v %in% RemovePDPList_Line)) {
          Test_ParDepPlots[[nam]][[v]] <- NULL
        } else if(v %in% RemovePDPList_Line) {
          Test_ParDepPlots[[nam]][[v]] <- NULL
        }
      }
    }
  
    # Add Plots to List per User Request
    if(!is.null(TestData) && !is.null(AddPDPList_Line)) {
      for(g in AddPDPList_Line) {
        if(is.numeric(TestData[[g]])) {
          
          # Need TestData
          TestData[, p1 := get(nam)]
          TestData[, paste0('Temp_', nam) := data.table::fifelse(Predict == eval(nam), 1, 0)]
          
          # Add Plots
          Test_ParDepPlots[[nam]][[g]] <- RemixAutoML::ParDepCalPlots(
            data = TestData, 
            PredictionColName = 'p1', 
            TargetColName = paste0('Temp_', nam), 
            IndepVar = g, 
            GraphType = 'calibration', 
            PercentileBucket = 0.05, 
            FactLevels = 10, 
            Function = function(x) mean(x, na.rm = TRUE), 
            DateColumn = NULL, 
            DateAgg_3D = FALSE)
          
          # Remove added cols
          data.table::set(TestData, j = c('p1', paste0('Temp_', nam)), value = NULL)
        }
      }
    }
  }
  
  ### TrainData ----
  
  # Plots to Add and Remove
  
  # Extra list mgt + par dep plot for multinomial
  
  # Numeric-Train: Starting batch of plots
  Train_ParDepPlots <- list()
  ParDepPlotsNames <- names(RemixOutput[['PlotList']])
  Train_ParDepPlotss <- gsub(pattern = 'Train_ParDepPlots_', replacement = '', x = ParDepPlotsNames[ParDepPlotsNames %like% 'Train_ParDepPlots_'])
  for(nam in Train_ParDepPlotss) {
    
    # Name to keep and remove
    Train_ParDepPlots[[nam]] <- RemixOutput$PlotList[[paste0('Train_ParDepPlots_', nam)]]

    # Remove Names
    A <- names(Train_ParDepPlots)
    B <- FeatureColumnNames
    RemovePDPList_Line <- setdiff(A, B)
    if(identical(character(0), RemovePDPList_Line)) RemovePDPList_Line <- NULL
    
    # Add Names
    AddPDPList_Line = setdiff(B, A)
    if(identical(character(0), AddPDPList_Line)) AddPDPList_Line <- NULL
    
    # Remove Plots from List per User Request
    if(!is.null(A)) {
      
      # Loop through all names of PDP List
      for(v in A) {
  
        # Remove Plots
        if(!is.null(TrainData) && (!is.numeric(TrainData[[v]]) || v %in% RemovePDPList_Line)) {
          Train_ParDepPlots[[nam]][[v]] <- NULL
        } else if(v %in% RemovePDPList_Line) {
          Train_ParDepPlots[[nam]][[v]] <- NULL
        }
      }
    }
  
    # Add Plots to List per User Request
    if(!is.null(AddPDPList_Line)) {
      for(g in AddPDPList_Line) {
        if(is.numeric(TrainData[[g]])) {
          
          # Need TestData
          TrainData[, p1 := get(nam)]
          TrainData[, paste0('Temp_', nam) := data.table::fifelse(Predict == eval(nam), 1, 0)]
          
          # Add Plots
          Train_ParDepPlots[[nam]][[g]] <- RemixAutoML::ParDepCalPlots(
            data = TrainData, 
            PredictionColName = 'p1', 
            TargetColName = paste0('Temp_', nam), 
            IndepVar = g, 
            GraphType = 'calibration', 
            PercentileBucket = 0.05, 
            FactLevels = 10, 
            Function = function(x) mean(x, na.rm = TRUE), 
            DateColumn = NULL, 
            DateAgg_3D = FALSE)
          
          # Remove added cols
          data.table::set(TrainData, j = c('p1', paste0('Temp_', nam)), value = NULL)
        }
      }
    }
  }
    
  ## Model_Evaluation_Metrics_CategoricalVariables ---- 
  
  ### Test Data ----
  
  # Name to keep and remove
  # setdiff(x = c(1,2,3,4,5), y = c(3,4,5,6,7))
  
  # Categorical-Test: Starting batch of plots
  Test_ParDepCatPlots <- list()
  ParDepCatPlotsNames <- names(RemixOutput[['PlotList']])
  Test_ParDepCatPlotss <- gsub(pattern = 'Test_ParDepPlots_', replacement = '', x = ParDepPlotsNames[ParDepPlotsNames %like% 'Test_ParDepPlots_'])
  for(nam in Test_ParDepPlotss) {
    
    # Name to keep and remove
    Test_ParDepCatPlots[[nam]] <- RemixOutput$PlotList[[paste0('Test_ParDepPlots_', nam)]]

    # Remove Names
    A <- names(Test_ParDepCatPlots)
    B <- FeatureColumnNames
    RemovePDPList_Bar <- setdiff(A, B)
    if(identical(character(0), RemovePDPList_Bar)) RemovePDPList_Bar <- NULL
    
    # Add Names
    AddPDPList_Bar = setdiff(B, A)
    if(identical(character(0), AddPDPList_Bar)) AddPDPList_Bar <- NULL

    # Remove Plots from List per User Request
    if(!is.null(A)) {
      
      # Loop through all names of PDP List
      for(v in A) {
  
        # Remove Plots
        if(!is.null(TestData) && (is.numeric(TestData[[v]]) || v %in% RemovePDPList_Bar)) {
          Test_ParDepCatPlots[[nam]][[v]] <- NULL
        } else if(v %in% RemovePDPList_Bar) {
          Test_ParDepCatPlots[[nam]][[v]] <- NULL
        }
      }
    }
    
    # Add Plots to List per User Request
    if(!is.null(AddPDPList_Bar) && !is.null(TestData)) {
      for(g in AddPDPList_Bar) {
        if(!is.numeric(TestData[[g]])) {
          
          # Need TestData
          TestData[, p1 := get(nam)]
          TestData[, paste0('Temp_', nam) := data.table::fifelse(Predict == eval(nam), 1, 0)]
  
          # Add Plots
          Test_ParDepCatPlots[[nam]][[g]] <- RemixAutoML::ParDepCalPlots(
            data = TestData, 
            PredictionColName = 'p1', 
            TargetColName = paste0('Temp_', nam), 
            IndepVar = g, 
            GraphType = 'calibration', 
            PercentileBucket = 0.05, 
            FactLevels = 10, 
            Function = function(x) mean(x, na.rm = TRUE), 
            DateColumn = NULL, 
            DateAgg_3D = FALSE)
          
          # Remove added cols
          data.table::set(TestData, j = c('p1', paste0('Temp_', nam)), value = NULL)
        }
      }
    }
  }
  
  ### Train Data ----
  
  # Name to keep and remove
  # setdiff(x = c(1,2,3,4,5), y = c(3,4,5,6,7))
  
  # Categorical-Test: Starting batch of plots
  Train_ParDepCatPlots <- list()
  ParDepCatPlotsNames <- names(RemixOutput[['PlotList']])
  Train_ParDepCatPlotss <- gsub(pattern = 'Test_ParDepPlots_', replacement = '', x = ParDepPlotsNames[ParDepPlotsNames %like% 'Test_ParDepPlots_'])
  for(nam in Train_ParDepCatPlotss) {
    
    # Name to keep and remove
    Train_ParDepCatPlots[[nam]] <- RemixOutput$PlotList[[paste0('Train_ParDepPlots_', nam)]]

    # Remove Names
    A <- names(Train_ParDepCatPlots)
    B <- FeatureColumnNames
    RemovePDPList_Bar <- setdiff(A, B)
    if(identical(character(0), RemovePDPList_Bar)) RemovePDPList_Bar <- NULL
    
    # Add Names
    AddPDPList_Bar = setdiff(B, A)
    if(identical(character(0), AddPDPList_Bar)) AddPDPList_Bar <- NULL

    # Remove Plots from List per User Request or due to type mismatch
    if(!is.null(A)) {
      
      # Loop through all names of PDP List
      for(v in A) {
  
        # Remove Plots
        if(!is.null(TrainData) && (is.numeric(TrainData[[v]]) || v %in% RemovePDPList_Bar)) {
          Train_ParDepCatPlots[[nam]][[v]] <- NULL
        } else if(v %in% RemovePDPList_Bar) {
          Train_ParDepCatPlots[[nam]][[v]] <- NULL
        }
      }
    }
    
    # Add Plots to List per User Request
    if(!is.null(AddPDPList_Bar) && !is.null(TrainData)) {
      for(g in AddPDPList_Bar) {
        if(!is.numeric(TrainData[[g]])) {
          
          # Need TrainData
          TrainData[, p1 := get(nam)]
          TrainData[, paste0('Temp_', nam) := data.table::fifelse(Predict == eval(nam), 1, 0)]
  
          # Add Plots
          Train_ParDepCatPlots[[nam]][[g]] <- RemixAutoML::ParDepCalPlots(
            data = TrainData, 
            PredictionColName = 'p1', 
            TargetColName = paste0('Temp_', nam), 
            IndepVar = g, 
            GraphType = 'calibration', 
            PercentileBucket = 0.05, 
            FactLevels = 10, 
            Function = function(x) mean(x, na.rm = TRUE), 
            DateColumn = NULL, 
            DateAgg_3D = FALSE)
          
          # Remove added cols
          data.table::set(TrainData, j = c('p1', paste0('Temp_', nam)), value = NULL)
        }
      }
    }
  }
}
```

```{r Generic_DataSets_And_MetaData, echo = FALSE}
if(is.null(RemixOutput)) {
  
  # DataSets
  if(is.null(TestData) && file.exists(file.path(SourcePath, paste0(ModelID, "_ValidationData.csv")))) {
    TestData <- data.table::fread(file = file.path(SourcePath, paste0(ModelID, "_ValidationData.csv")))
  }
  # Validate
  if(is.null(ValidationData) && file.exists(file.path(SourcePath, paste0(ModelID, "_ValData.csv")))) {
    ValidationDataData <- data.table::fread(file = file.path(SourcePath, paste0(ModelID, "_ValData.csv")))
  }
  # Train
  if(is.null(TrainData) && file.exists(file.path(SourcePath, paste0(ModelID, "_TrainData.csv")))) {
    TrainData <- data.table::fread(file = file.path(SourcePath, paste0(ModelID, "_TrainData.csv")))
  }
  
  # Meta info
  TargetColumnName <- TargetColumnName
  PredictionColumnName <- PredictionColumnName
  if(is.null(FeatureColumnNames) && !is.null(TestData)) {
    FeatureColumnNames <- names(TestData)[!names(TestData) %in% c(TargetColumnName, PredictionColumnName)]
  }
  if(is.null(FeatureColumnNames) && !is.null(ValidationData)) {
    FeatureColumnNames <- names(ValidationData)[!names(ValidationData) %in% c(TargetColumnName, PredictionColumnName)]
  }
  if(is.null(FeatureColumnNames) && !is.null(TrainData)) {
    FeatureColumnNames <- names(TrainData)[!names(TrainData) %in% c(TargetColumnName, PredictionColumnName)]
  }
  if(is.list(FeatureColumnNames) || data.table::is.data.table(FeatureColumnNames)) {
    FeatureColumnNames <- FeatureColumnNames[[1L]]
  }
  if(is.null(DateColumnName) && !is.null(RemixOutput[['ArgsList']][['PrimaryDateColumn']])) {
    DateColumnName <- RemixOutput[['ArgsList']][['PrimaryDateColumn']]
  } else {
    DateColumnName <- NULL
  }
  
  # Target levels
  counter <- 1L
  DataList <- list()
  DataList[['TrainData']] <- TrainData
  DataList[['ValidationData']] <- ValidationData
  DataList[['TestData']] <- TestData
  for(d in names(DataList)) {
    DataList[[d]] <- DataList[[d]][, .N, by = c(eval(TargetColumnName))]
  }
  
  # Keep for functions that need the table
  TargetLevels.. <- data.table::rbindlist(DataList)
  data.table::setnames(TargetLevels.., TargetColumnName, 'OriginalLevels')
  TargetLevels.. <- TargetLevels..[, list(N = sum(N)), by = 'OriginalLevels']
  TargetLevels <- sort(TargetLevels..[, unique(OriginalLevels)])
  rm(DataList)
}
```

```{r Generic_Model_MetaData, echo = FALSE}
if(is.null(RemixOutput)) {

  # Model MetaData ----

  ## Model_MetaData_Parameters ----
  if(!is.null(SourcePath) && !is.null(ModelID)) {
    if(file.exists(file.path(SourcePath, paste0(ModelID, "_ArgsList.Rdata")))) {
      load(file.path(SourcePath, paste0(ModelID, "_ArgsList.Rdata")))
    }
  } else {
    ArgsList <- NULL
  }

  ## Model_MetaData_GridMetrics ----
  if(!is.null(SourcePath) && !is.null(ModelID)) {
    if(file.exists(file.path(SourcePath, paste0(ModelID, "_GridMetrics.csv")))) {
      GridMetrics <- data.table::fread(file = file.path(SourcePath, paste0(ModelID, "_GridMetrics.csv")))
    } else {
      GridMetrics <- NULL
    }
  } else {
    GridMetrics <- NULL
  }
}
```
  
```{r Generic_Evaluation_Metrics, echo = FALSE}
if(is.null(RemixOutput)) {
  
  # Evaluation Metrics ----
  
  ## MultiClass Metrics ----
  Test_MultiClassMetrics <- data.table::fread(file = file.path(SourcePath, paste0(ModelID, "_Test_EvaluationMetrics.csv")))
  Train_MultiClassMetrics <- data.table::fread(file = file.path(SourcePath, paste0(ModelID, "_Train_EvaluationMetrics.csv")))

  # Update Colnames
  if(!is.null(Test_MultiClassMetrics)) Test_MultiClassMetrics[, Data_Source := 'Test']
  if(!is.null(Train_MultiClassMetrics)) Train_MultiClassMetrics[, Data_Source := 'Train']

  # CatBoost only
  if(is.null(Test_MultiClassMetrics) && is.null(Train_MultiClassMetrics)) {
    All_MultiClassMetrics <- NULL
  } else if(!is.null(Test_MultiClassMetrics) && !is.null(Train_MultiClassMetrics)) {
    All_MultiClassMetrics <- data.table::rbindlist(list(
      Test_MultiClassMetrics, Train_MultiClassMetrics))
  } else if(is.null(Test_MultiClassMetrics) && !is.null(Train_MultiClassMetrics)) {
    All_MultiClassMetrics <- Train_MultiClassMetrics
  } else if(!is.null(Test_MultiClassMetrics) && is.null(Train_MultiClassMetrics)) {
    All_MultiClassMetrics <- Test_MultiClassMetrics
  } else {
    All_MultiClassMetrics <- NULL
  }
  
    
  ## Model_Evaluation_Metrics ----
  
  ### Test
  if(!is.null(TestData) && !file.exists(file.path(SourcePath, paste0(ModelID, "_Test_EvaluationMetrics.csv")))) {
    Test_EvalMetrics <- RemixAutoML:::MultiClassMetrics(
      ModelClass='catboost',
      DataType = 'Test',
      SaveModelObjects.=FALSE,
      ValidationData.=TestData,
      PredictData.=NULL,
      TrainOnFull.=FALSE,
      TargetColumnName.=TargetColumnName,
      TargetLevels.=TargetLevels..,
      ModelID.=ModelID,
      model_path.=NULL,
      metadata_path.=NULL)
  } else if(file.exists(file.path(SourcePath, paste0(ModelID, "_Test_EvaluationMetrics.csv")))) {
    Test_EvalMetrics <- data.table::fread(file = file.path(SourcePath, paste0(ModelID, "_Test_EvaluationMetrics.csv")))
  } else {
    Test_EvalMetrics <- NULL
  }

  ### Train
  if(!is.null(TrainData) && !file.exists(file.path(SourcePath, paste0(ModelID, "_Train_EvaluationMetrics.csv")))) {
    Train_EvalMetrics <- RemixAutoML:::MultiClassMetrics(
      ModelClass='catboost',
      DataType = 'Test',
      SaveModelObjects.=FALSE,
      ValidationData.=TrainData,
      PredictData.=NULL,
      TrainOnFull.=FALSE,
      TargetColumnName.=TargetColumnName,
      TargetLevels.=TargetLevels..,
      ModelID.=ModelID,
      model_path.=NULL,
      metadata_path.=NULL)
  } else if(file.exists(file.path(SourcePath, paste0(ModelID, "_Train_EvaluationMetrics.csv")))) {
    Train_EvalMetrics <- data.table::fread(file = file.path(SourcePath, paste0(ModelID, "_Train_EvaluationMetrics.csv")))
  } else {
    Train_EvalMetrics <- NULL
  }
  
  ## Model_VarImportanceTable ----
  if(tolower(Algo) == 'catboost') {
    if(file.exists(file.path(SourcePath, paste0(ModelID, "_Test_Importance_VariableImportance.csv")))) {
      Test_Importance <- data.table::fread(file = file.path(SourcePath, paste0(ModelID, "_Test_Importance_VariableImportance.csv")))
    } else {
      Test_Importance <- NULL
    }
    if(file.exists(file.path(SourcePath, paste0(ModelID, "_Validation_Importance_VariableImportance.csv")))) {
      Validation_Importance <- data.table::fread(file = file.path(SourcePath, paste0(ModelID, "_Validation_Importance_VariableImportance.csv")))
    } else {
      Validation_Importance <- NULL
    }
    if(file.exists(file.path(SourcePath, paste0(ModelID, "_Train_Importance_VariableImportance.csv")))) {
      Train_Importance <- data.table::fread(file = file.path(SourcePath, paste0(ModelID, "_Train_Importance_VariableImportance.csv")))
    } else {
      Train_Importance <- NULL
    }
    
    # Update Colnames
    if(!is.null(Test_Importance)) data.table::setnames(Test_Importance, old = 'Importance', new = 'Test_Importance', skip_absent = TRUE)
    if(!is.null(Validation_Importance)) data.table::setnames(Validation_Importance, old = 'Importance', new = 'Validation_Importance', skip_absent = TRUE)
    if(!is.null(Train_Importance)) data.table::setnames(Train_Importance, old = 'Importance', new = 'Train_Importance', skip_absent = TRUE)

    # CatBoost only
    if(is.null(Test_Importance) && is.null(Validation_Importance) && is.null(Train_Importance)) {
      All_Importance <- NULL
    } else if(!is.null(Test_Importance) && !is.null(Validation_Importance) && !is.null(Train_Importance)) {
      All_Importance <- merge(Test_Importance, Validation_Importance, by = 'Variable', all = TRUE)
      All_Importance <- merge(All_Importance, Train_Importance, by = 'Variable', all = TRUE)
    } else if(!is.null(Test_Importance) && !is.null(Validation_Importance) && is.null(Train_Importance)) {
      All_Importance <- merge(Test_Importance, Validation_Importance, by = 'Variable', all = TRUE)
    } else if(!is.null(Test_Importance) && is.null(Validation_Importance) && !is.null(Train_Importance)) {
      All_Importance <- merge(Test_Importance, Train_Importance, by = 'Variable', all = TRUE)
    } else if(is.null(Test_Importance) && !is.null(Validation_Importance) && !is.null(Train_Importance)) {
      All_Importance <- merge(Validation_Importance, Train_Importance, by = 'Variable', all = TRUE)
    } else if(is.null(Test_Importance) && is.null(Validation_Importance) && !is.null(Train_Importance)) {
      All_Importance <- Train_Importance
    } else if(is.null(Test_Importance) && !is.null(Validation_Importance) && is.null(Train_Importance)) {
      All_Importance <- Validation_Importance
    } else if(!is.null(Test_Importance) && is.null(Validation_Importance) && is.null(Train_Importance)) {
      All_Importance <- Test_Importance
    } else {
      All_Importance <- NULL
    }
    
  } else {
    
    # Encoding-based Models + Generic Connector
    if(is.null(Test_Importance_dt)) {
      Test_Importance <- NULL
    } else {
      Test_Importance <- Test_Importance_dt
    }
    if(is.null(Validation_Importance_dt)) {
      Validation_Importance <- NULL
    } else {
      Validation_Importance <- Validation_Importance_dt
    }
    if(is.null(Train_Importance_dt)) {
      Train_Importance <- NULL
    } else {
      Train_Importance <- Train_Importance_dt
    }
    
    # Update Colnames
    if(!is.null(Test_Importance)) data.table::setnames(Test_Importance, old = 'Importance', new = 'Test_Importance', skip_absent = TRUE)
    if(!is.null(Validation_Importance)) data.table::setnames(Validation_Importance, old = 'Importance', new = 'Validation_Importance', skip_absent = TRUE)
    if(!is.null(Train_Importance)) data.table::setnames(Train_Importance, old = 'Importance', new = 'Train_Importance', skip_absent = TRUE)

    # CatBoost only
    if(is.null(Test_Importance) && is.null(Validation_Importance) && is.null(Train_Importance)) {
      All_Importance <- NULL
    } else if(!is.null(Test_Importance) && !is.null(Validation_Importance) && !is.null(Train_Importance)) {
      All_Importance <- merge(Test_Importance, Validation_Importance, by = 'Variable', all = TRUE)
      All_Importance <- merge(All_Importance, Train_Importance, by = 'Variable', all = TRUE)
    } else if(!is.null(Test_Importance) && !is.null(Validation_Importance) && is.null(Train_Importance)) {
      All_Importance <- merge(Test_Importance, Validation_Importance, by = 'Variable', all = TRUE)
    } else if(!is.null(Test_Importance) && is.null(Validation_Importance) && !is.null(Train_Importance)) {
      All_Importance <- merge(Test_Importance, Train_Importance, by = 'Variable', all = TRUE)
    } else if(is.null(Test_Importance) && !is.null(Validation_Importance) && !is.null(Train_Importance)) {
      All_Importance <- merge(Validation_Importance, Train_Importance, by = 'Variable', all = TRUE)
    } else if(is.null(Test_Importance) && is.null(Validation_Importance) && !is.null(Train_Importance)) {
      All_Importance <- Train_Importance
    } else if(is.null(Test_Importance) && !is.null(Validation_Importance) && is.null(Train_Importance)) {
      All_Importance <- Validation_Importance
    } else if(!is.null(Test_Importance) && is.null(Validation_Importance) && is.null(Train_Importance)) {
      All_Importance <- Test_Importance
    } else {
      All_Importance <- NULL
    }
    
  }
  
  ## Model_IntImportanceTable ----
  if(tolower(Algo) == 'catboost') {
    if(file.exists(file.path(SourcePath, paste0(ModelID, "_Test_Interaction_Interaction.csv")))) {
      Test_Interaction <- data.table::fread(file = file.path(SourcePath, paste0(ModelID, "_Test_Interaction_Interaction.csv")))
    } else {
      Test_Interaction <- NULL
    }
    if(file.exists(file.path(SourcePath, paste0(ModelID, "_Validation_Interaction_Interaction.csv")))) {
      Validation_Interaction <- data.table::fread(file = file.path(SourcePath, paste0(ModelID, "_Validation_Interaction_Interaction.csv")))
    } else {
      Validation_Interaction <- NULL
    }
    if(file.exists(file.path(SourcePath, paste0(ModelID, "_Train_Interaction_Interaction.csv")))) {
      Train_Interaction <- data.table::fread(file = file.path(SourcePath, paste0(ModelID, "_Train_Interaction_Interaction.csv")))
    } else {
      Train_Interaction <- NULL
    }
    
    # Update Colnames
    if(!is.null(Test_Interaction)) data.table::setnames(Test_Interaction, old = 'score', new = 'Test_Importance', skip_absent = TRUE)
    if(!is.null(Validation_Interaction)) data.table::setnames(Validation_Interaction, old = 'score', new = 'Validation_Importance', skip_absent = TRUE)
    if(!is.null(Train_Interaction)) data.table::setnames(Train_Interaction, old = 'score', new = 'Train_Importance', skip_absent = TRUE)
    
    # CatBoost only
    if(is.null(Test_Interaction) && is.null(Validation_Interaction) && is.null(Train_Interaction)) {
      All_Interaction <- NULL
    } else if(!is.null(Test_Interaction) && !is.null(Validation_Interaction) && !is.null(Train_Interaction)) {
      All_Interaction <- merge(Test_Interaction, Validation_Interaction, by = c('Features1','Features2'), all = TRUE)
      All_Interaction <- merge(All_Interaction, Train_Interaction, by = c('Features1','Features2'), all = TRUE)
      data.table::setorderv(x = All_Interaction, cols = names(All_Interaction)[3L], order = -1)
    } else if(!is.null(Test_Interaction) && !is.null(Validation_Interaction) && is.null(Train_Interaction)) {
      All_Interaction <- merge(Test_Interaction, Validation_Interaction, by = c('Features1','Features2'), all = TRUE)
      data.table::setorderv(x = All_Interaction, cols = names(All_Interaction)[3L], order = -1)
    } else if(!is.null(Test_Interaction) && is.null(Validation_Interaction) && !is.null(Train_Interaction)) {
      All_Interaction <- merge(Test_Interaction, Train_Interaction, by = c('Features1','Features2'), all = TRUE)
      data.table::setorderv(x = All_Interaction, cols = names(All_Interaction)[3L], order = -1)
    } else if(is.null(Test_Interaction) && !is.null(Validation_Interaction) && !is.null(Train_Interaction)) {
      All_Interaction <- merge(Validation_Interaction, Train_Interaction, by = c('Features1','Features2'), all = TRUE)
      data.table::setorderv(x = All_Interaction, cols = names(All_Interaction)[3L], order = -1)
    } else if(is.null(Test_Interaction) && is.null(Validation_Interaction) && !is.null(Train_Interaction)) {
      All_Interaction <- Train_Interaction
    } else if(is.null(Test_Interaction) && !is.null(Validation_Interaction) && is.null(Train_Interaction)) {
      All_Interaction <- Validation_Interaction
    } else if(!is.null(Test_Interaction) && is.null(Validation_Interaction) && is.null(Train_Interaction)) {
      All_Interaction <- Test_Interaction
    } else {
      All_Interaction <- NULL
    }
    
  } else {
    
    # Encoding-based Models + Generic Connector
    if(is.null(Test_Interaction_dt)) {
      Test_Interaction <- NULL
    } else {
      Test_Interaction <- Test_Interaction_dt
    }
    if(is.null(Validation_Interaction_dt)) {
      Validation_Interaction <- NULL
    } else {
      Validation_Interaction <- Validation_Interaction_dt
    }
    if(is.null(Train_Interaction_dt)) {
      Train_Interaction <- NULL
    } else {
      Train_Interaction <- Train_Interaction_dt
    }
    
    # Update Colnames
    if(!is.null(Test_Interaction)) data.table::setnames(Test_Interaction, old = 'score', new = 'Test_Importance', skip_absent = TRUE)
    if(!is.null(Validation_Interaction)) data.table::setnames(Validation_Interaction, old = 'score', new = 'Validation_Importance', skip_absent = TRUE)
    if(!is.null(Train_Interaction)) data.table::setnames(Train_Interaction, old = 'score', new = 'Train_Importance', skip_absent = TRUE)
    
    # CatBoost only
    if(is.null(Test_Interaction) && is.null(Validation_Interaction) && is.null(Train_Interaction)) {
      All_Interaction <- NULL
    } else if(!is.null(Test_Interaction) && !is.null(Validation_Interaction) && !is.null(Train_Interaction)) {
      All_Interaction <- merge(Test_Interaction, Validation_Interaction, by = c('Features1','Features2'), all = TRUE)
      All_Interaction <- merge(All_Interaction, Train_Interaction, by = c('Features1','Features2'), all = TRUE)
      data.table::setorderv(x = All_Interaction, cols = names(All_Interaction)[3L], order = -1)
    } else if(!is.null(Test_Interaction) && !is.null(Validation_Interaction) && is.null(Train_Interaction)) {
      All_Interaction <- merge(Test_Interaction, Validation_Interaction, by = c('Features1','Features2'), all = TRUE)
      data.table::setorderv(x = All_Interaction, cols = names(All_Interaction)[3L], order = -1)
    } else if(!is.null(Test_Interaction) && is.null(Validation_Interaction) && !is.null(Train_Interaction)) {
      All_Interaction <- merge(Test_Interaction, Train_Interaction, by = c('Features1','Features2'), all = TRUE)
      data.table::setorderv(x = All_Interaction, cols = names(All_Interaction)[3L], order = -1)
    } else if(is.null(Test_Interaction) && !is.null(Validation_Interaction) && !is.null(Train_Interaction)) {
      All_Interaction <- merge(Validation_Interaction, Train_Interaction, by = c('Features1','Features2'), all = TRUE)
      data.table::setorderv(x = All_Interaction, cols = names(All_Interaction)[3L], order = -1)
    } else if(is.null(Test_Interaction) && is.null(Validation_Interaction) && !is.null(Train_Interaction)) {
      All_Interaction <- Train_Interaction
    } else if(is.null(Test_Interaction) && !is.null(Validation_Interaction) && is.null(Train_Interaction)) {
      All_Interaction <- Validation_Interaction
    } else if(!is.null(Test_Interaction) && is.null(Validation_Interaction) && is.null(Train_Interaction)) {
      All_Interaction <- Test_Interaction
    } else {
      All_Interaction <- NULL
    }
    
  }
}
```

```{r Generic_Evaluation_Plots, echo = FALSE}
if(is.null(RemixOutput)) {
  
  options(warn = -1)
  
  # Evaluation Plots ----
    
  ## EvaluationPlots_CalibrationPlot ----
  
  ### Test ----
  if(!is.null(TestData)) {
    Test_EvaluationPlot <- list()
    for(tarlevel in TargetLevels) {
      TestData[, p1 := get(tarlevel)]
      TestData[, paste0('Temp_', tarlevel) := data.table::fifelse(Predict == eval(tarlevel), 1, 0)]
      Test_EvaluationPlot[[tarlevel]] <- suppressMessages(RemixAutoML::EvalPlot(
        data = data.table::copy(TestData), 
        PredictionColName = 'p1',
        TargetColName = paste0('Temp_', tarlevel), 
        GraphType = 'calibration',
        PercentileBucket = 0.05, 
        aggrfun = function(x) mean(x, na.rm = TRUE)))
    }
    
  } else {
    Test_EvaluationPlot <- NULL
  }

  ### Train ----
  if(!is.null(TrainData)) {
    Train_EvaluationPlot <- list()
    for(tarlevel in TargetLevels) {
      TrainData[, p1 := get(tarlevel)]
      TrainData[, paste0('Temp_', tarlevel) := data.table::fifelse(Predict == eval(tarlevel), 1, 0)]
      Train_EvaluationPlot[[tarlevel]] <- suppressMessages(RemixAutoML::EvalPlot(
        data = data.table::copy(TrainData),
        PredictionColName = 'p1',
        TargetColName = paste0('Temp_', tarlevel),
        GraphType = 'calibration',
        PercentileBucket = 0.05,
        aggrfun = function(x) mean(x, na.rm = TRUE)))
    }
  } else {
    Train_EvaluationPlot <- NULL
  }
  
  ## EvaluationPlots_ROC_Plot ----
  
  ### TestData ----
  if(!is.null(TestData)) {
    Test_ROCPlot <- list()
    for(tarlevel in TargetLevels) {
      TestData[, p1 := get(tarlevel)]
      TestData[, paste0('Temp_', tarlevel) := data.table::fifelse(Predict == eval(tarlevel), 1, 0)]
      Test_ROCPlot[[tarlevel]] <- suppressMessages(RemixAutoML::ROCPlot(
        data = TestData,
        TargetName = paste0('Temp_', tarlevel),
        SavePlot = FALSE,
        Name = ModelID,
        metapath = NULL,
        modelpath = NULL))
    }
  } else {
    Test_ROCPlot <- NULL
  }
  
  ### TrainData ----
  if(!is.null(TrainData)) {
    Train_ROCPlot <- list()
    for(tarlevel in TargetLevels) {
      TrainData[, p1 := get(tarlevel)]
      TrainData[, paste0('Temp_', tarlevel) := data.table::fifelse(Predict == eval(tarlevel), 1, 0)]
      Train_ROCPlot[[tarlevel]] <- suppressMessages(RemixAutoML::ROCPlot(
        data = TrainData,
        TargetName = paste0('Temp_', tarlevel),
        SavePlot = FALSE,
        Name = ModelID,
        metapath = NULL,
        modelpath = NULL))
    }
  }
  
  ## EvaluationPlots_GainsPlots ----
  
  ## Test_CumGainsChart ----
  if(!is.null(TestData)) {
    Test_CumGainsChart <- list()
    for(tarlevel in TargetLevels) {
      TestData[, p1 := get(tarlevel)]
      TestData[, paste0('Temp_', tarlevel) := data.table::fifelse(Predict == eval(tarlevel), 1, 0)]
      Test_CumGainsChart[[tarlevel]] <- suppressMessages(RemixAutoML::CumGainsChart(
        data = data.table::copy(TestData),
        PredictedColumnName = 'p1',
        TargetColumnName = paste0('Temp_', tarlevel),
        NumBins = 20,
        SavePlot = FALSE,
        Name = ModelID,
        metapath = NULL,
        modelpath = NULL))
    }
  } else {
    Test_CumGainsChart <- NULL
  }

  ## Train_CumGainsChart ----
  if(!is.null(TrainData)) {
    Train_CumGainsChart <- list()
    for(tarlevel in TargetLevels) {
      TrainData[, p1 := get(tarlevel)]
      TrainData[, paste0('Temp_', tarlevel) := data.table::fifelse(Predict == eval(tarlevel), 1, 0)]
      Train_CumGainsChart[[tarlevel]] <- suppressMessages(RemixAutoML::CumGainsChart(
        data = data.table::copy(TrainData),
        PredictedColumnName = 'p1',
        TargetColumnName = paste0('Temp_', tarlevel),
        NumBins = 20,
        SavePlot = FALSE,
        Name = ModelID,
        metapath = NULL,
        modelpath = NULL))
    }
  } else {
    Train_CumGainsChart <- NULL
  }
}
```

```{r Generic_Model_Interpretation, echo = FALSE}
if(is.null(RemixOutput)) {
  
  # Model Interpretation ----
  
  ## Model_Evaluation_Metrics_NumericVariables ----
  
  ### Test ----

  # Name to keep and remove
  # setdiff(x = c(1,2,3,4,5), y = c(3,4,5,6,7))
  Test_ParDepPlots <- list()
  for(nam in TargetLevels) {
    
    # Starting batch of plots
    if(file.exists(file.path(SourcePath, paste0(ModelID, "_Test_ParDepPlots_", nam, ".Rdata")))) {
      load(file = file.path(SourcePath, paste0(ModelID, "_Test_ParDepPlots_", nam, ".Rdata")))
      Test_ParDepPlotss <- ParDepPlots
      for(n in names(Test_ParDepPlotss)) Test_ParDepPlots[[nam]][[n]] <- Test_ParDepPlotss[[n]]
      rm(ParDepPlots, Test_ParDepPlotss)
    }

    # Remove Names
    A <- names(Test_ParDepPlots[[nam]])
    B <- FeatureColumnNames
    RemovePDPList_Line <- setdiff(A, B)
    if(identical(character(0), RemovePDPList_Line)) RemovePDPList_Line <- NULL
    
    # Add Names
    AddPDPList_Line = setdiff(B, A)
    if(identical(character(0), AddPDPList_Line)) AddPDPList_Line <- NULL
  
    # Remove Plots
    if(!is.null(A)) {
      for(v in A) {
  
        # Remove
        if(!is.null(TestData) && (!is.numeric(TestData[[v]]) || v %in% RemovePDPList_Line)) {
          Test_ParDepPlots[[nam]][[v]] <- NULL
        } else if(v %in% RemovePDPList_Line) {
          Test_ParDepPlots[[nam]][[v]] <- NULL
        }
      }
    }
    
    # Add Plots
    if(!is.null(AddPDPList_Line) && !is.null(TestData)) {
      for(g in AddPDPList_Line) {
        if(is.numeric(TestData[[g]])) {
  
          # Add
          TestData[, p1 := get(tarlevel)]
          TestData[, paste0('Temp_', tarlevel) := data.table::fifelse(Predict == eval(tarlevel), 1, 0)]
          Test_ParDepPlots[[nam]][[g]] <- RemixAutoML::ParDepCalPlots(
            data = TestData,
            PredictionColName = 'p1',
            TargetColName = paste0('Temp_', tarlevel),
            IndepVar = g,
            GraphType = 'calibration',
            PercentileBucket = 0.05,
            FactLevels = 10,
            Function = function(x) mean(x, na.rm = TRUE),
            DateColumn = NULL,
            DateAgg_3D = FALSE)
        }
      }
    }
  }
  
  
  ### Train ----

  # Name to keep and remove
  # setdiff(x = c(1,2,3,4,5), y = c(3,4,5,6,7))
  Train_ParDepPlots <- list()
  for(nam in TargetLevels) {

    # Starting batch of plots
    if(file.exists(file.path(SourcePath, paste0(ModelID, "_Train_ParDepPlots_", nam, ".Rdata")))) {
      load(file = file.path(SourcePath, paste0(ModelID, "_Train_ParDepPlots_", nam, ".Rdata")))
      Train_ParDepPlotss <- ParDepPlots
      for(n in names(Train_ParDepPlotss)) Train_ParDepPlots[[nam]][[n]] <- Train_ParDepPlotss[[n]]
      rm(ParDepPlots, Train_ParDepPlotss)
    }
    
    # Remove Names
    A <- names(Train_ParDepPlots[[nam]])
    B <- FeatureColumnNames
    RemovePDPList_Line <- setdiff(A, B)
    if(identical(character(0), RemovePDPList_Line)) RemovePDPList_Line <- NULL
    
    # Add Names
    AddPDPList_Line = setdiff(B, A)
    if(identical(character(0), AddPDPList_Line)) AddPDPList_Line <- NULL
  
    # Remove Plots
    if(!is.null(A)) {
      for(v in A) {
  
        # Remove
        if(!is.null(TrainData) && (!is.numeric(TrainData[[v]]) || v %in% RemovePDPList_Line)) {
          Train_ParDepPlots[[nam]][[v]] <- NULL
        } else if(v %in% RemovePDPList_Line) {
          Train_ParDepPlots[[nam]][[v]] <- NULL
        }
      }
    }
    
    # Add Plots
    if(!is.null(AddPDPList_Line) && !is.null(TrainData)) {
      for(g in AddPDPList_Line) {
        if(is.numeric(TrainData[[g]])) {
  
          # Add
          TrainData[, p1 := get(tarlevel)]
          TrainData[, paste0('Temp_', tarlevel) := data.table::fifelse(Predict == eval(tarlevel), 1, 0)]
          Train_ParDepPlots[[nam]][[g]] <- RemixAutoML::ParDepCalPlots(
            data = TrainData,
            PredictionColName = 'p1',
            TargetColName = paste0('Temp_', tarlevel),
            IndepVar = g,
            GraphType = 'calibration',
            PercentileBucket = 0.05,
            FactLevels = 10,
            Function = function(x) mean(x, na.rm = TRUE),
            DateColumn = NULL,
            DateAgg_3D = FALSE)
        }
      }
    }
  }

  ## Model_Evaluiation_Metrics_CategoricalVariables ----
  
  ### Test ----

  # Name to keep and remove
  # setdiff(x = c(1,2,3,4,5), y = c(3,4,5,6,7))
  Test_ParDepCatPlots <- list()
  for(nam in TargetLevels) {
    
    # Starting batch of plots
    if(file.exists(file.path(SourcePath, paste0(ModelID, "_Test_ParDepPlots_", nam, ".Rdata")))) {
      load(file = file.path(SourcePath, paste0(ModelID, "_Test_ParDepPlots_", nam, ".Rdata")))
      Test_ParDepCatPlotss <- ParDepPlots
      for(n in names(Test_ParDepCatPlotss)) Test_ParDepCatPlots[[nam]][[n]] <- Test_ParDepCatPlotss[[n]]
      rm(ParDepPlots, Test_ParDepCatPlotss)
    }

    # Remove Names
    A <- names(Test_ParDepCatPlots[[nam]])
    B <- FeatureColumnNames
    RemovePDPList_Bar <- setdiff(A, B)
    if(identical(character(0), RemovePDPList_Bar)) RemovePDPList_Bar <- NULL
    
    # Add Names
    AddPDPList_Bar = setdiff(B, A)
    if(identical(character(0), AddPDPList_Bar)) AddPDPList_Bar <- NULL
  
    # Remove Plots
    if(!is.null(A)) {
      for(v in A) {
      
        # Remove
        if(!is.null(TestData) && (is.numeric(TestData[[v]]) || v %in% RemovePDPList_Bar)) {
          Test_ParDepCatPlots[[nam]][[v]] <- NULL
        } else if(v %in% RemovePDPList_Bar) {
          Test_ParDepCatPlots[[nam]][[v]] <- NULL
        }
      }
    }
    
    # Add Plots
    if(!is.null(TestData) && !is.null(AddPDPList_Bar)) {
      for(g in AddPDPList_Bar) {
        if(!is.numeric(TestData[[g]])) {
  
          # Add
          TestData[, p1 := get(tarlevel)]
          TestData[, paste0('Temp_', tarlevel) := data.table::fifelse(Predict == eval(tarlevel), 1, 0)]
          Test_ParDepCatPlots[[nam]][[g]] <- RemixAutoML::ParDepCalPlots(
            data = data.table::copy(TestData), 
            PredictionColName = 'p1',
            TargetColName = paste0('Temp_', tarlevel),
            IndepVar = g,
            GraphType = 'calibration',
            PercentileBucket = 0.05,
            FactLevels = 10,
            Function = function(x) mean(x, na.rm = TRUE),
            DateColumn = NULL,
            DateAgg_3D = FALSE)
        }
      }
    }
  }


  ### Train ----

  # Name to keep and remove
  # setdiff(x = c(1,2,3,4,5), y = c(3,4,5,6,7))
  Train_ParDepCatPlots <- list()
  for(nam in TargetLevels) {
    
    # Starting batch of plots
    if(file.exists(file.path(SourcePath, paste0(ModelID, "_Train_ParDepPlots_", nam, ".Rdata")))) {
      load(file = file.path(SourcePath, paste0(ModelID, "_Train_ParDepPlots_", nam, ".Rdata")))
      Train_ParDepCatPlotss <- ParDepPlots
      for(n in names(Train_ParDepCatPlotss)) Train_ParDepCatPlots[[nam]][[n]] <- Train_ParDepCatPlotss[[n]]
      rm(ParDepPlots, Train_ParDepCatPlotss)
    }

    # Remove Names
    A <- names(Train_ParDepCatPlots[[nam]])
    B <- FeatureColumnNames
    RemovePDPList_Bar <- setdiff(A, B)
    if(identical(character(0), RemovePDPList_Bar)) RemovePDPList_Bar <- NULL
    
    # Add Names
    AddPDPList_Bar <- setdiff(B, A)
    if(identical(character(0), AddPDPList_Bar)) AddPDPList_Bar <- NULL
  
    # Remove Plots
    if(!is.null(AddPDPList_Bar)) {
      for(v in A) {
      
        # Remove
        if(!is.null(TrainData) && (is.numeric(TrainData[[v]]) || v %in% RemovePDPList_Bar)) {
          Train_ParDepCatPlots[[nam]][[v]] <- NULL
        } else if(v %in% RemovePDPList_Bar) {
          Train_ParDepCatPlots[[nam]][[v]] <- NULL
        }
      }
    }
  
    # Add Plots
    if(!is.null(TrainData) && !is.null(AddPDPList_Bar)) {
      for(g in AddPDPList_Bar) {
        if(!is.numeric(TrainData[[g]])) {
  
          # Add
          TrainData[, p1 := get(tarlevel)]
          TrainData[, paste0('Temp_', tarlevel) := data.table::fifelse(Predict == eval(tarlevel), 1, 0)]
          Train_ParDepCatPlots[[nam]][[g]] <- RemixAutoML::ParDepCalPlots(
            data = TrainData,
            PredictionColName = 'p1',
            TargetColName = paste0('Temp_', tarlevel),
            IndepVar = g,
            GraphType = 'calibration',
            PercentileBucket = 0.05,
            FactLevels = 10,
            Function = function(x) mean(x, na.rm = TRUE),
            DateColumn = NULL,
            DateAgg_3D = FALSE)
        }
      }
    }
  }
  
  ### Remove Non-TargetLevel-plots from list ----
  # for(t in names(Test_ParDepPlots)) {
  #   if(!t %in% TargetLevels) Test_ParDepPlots[[t]] <- NULL
  # }
  # for(t in names(Train_ParDepPlots)) {
  #   if(!t %in% TargetLevels) Train_ParDepPlots[[t]] <- NULL
  # }
  # for(t in names(Test_ParDepCatPlots)) {
  #   if(!t %in% TargetLevels) Test_ParDepCatPlots[[t]] <- NULL
  # }
  # for(t in names(Train_ParDepCatPlots)) {
  #   if(!t %in% TargetLevels) Train_ParDepCatPlots[[t]] <- NULL
  # }
}
```


Model Evaluation and Insights:

- Provide a wide range of output to investigate high level performance and insights

- Deliver high quality report design layout to reduce time to delivery of info

This report is for investigating model performance from a variety of perspectives. Each section has its own content that can be viewed if expanded. Output exists for both TestData (out of sample) and TrainData (some cases can have ValidationData results as well) for comparison purposes.

# <font size="6">Evaluation Metrics</font>
<p>

<details><summary>Expand to view content</summary>
<p>



##  <font size="5">MultiClass Metrics Tables</font>

<details><summary>MultiClass Metrics</summary>
<p>

```{r Model_MultiClass_Metrics, echo=FALSE}
if(!is.null(All_MultiClassMetrics)) {
  All_MultiClassMetrics <- All_MultiClassMetrics[Data_Source != 'Train']
  data.table::setcolorder(All_MultiClassMetrics, c(3L, 1L, 2L))
  print(knitr::kable(All_MultiClassMetrics))
} else {
  print('All_MultiClassMetrics is NULL')
}
```

</details>
</p>




##  <font size="5">Binary Metrics 1 vs All Tables</font>

<details><summary>Model Metrics Tables</summary>
<p>


### **TestData**
<p>

<details><summary>Performance Metrics</summary>
<p>

```{r Model_Evaluation_Metrics, echo=FALSE}
if(!is.null(Test_EvalMetrics)) {
  for(nam in names(Test_EvalMetrics)) {
    print(nam)
    print(knitr::kable(Test_EvalMetrics[[nam]]))
  }
} else {
  print('Test_EvalMetrics is NULL')
}
```

</details>
</p>


### **TrainData + ValidationData**
<p>

<details><summary>Performance Metrics</summary>
<p>

```{r Model_Evaluation_Metrics_Train, echo=FALSE}
if(!is.null(Train_EvalMetrics)) {
  for(nam in names(Train_EvalMetrics)) {
    print(nam)
    print(knitr::kable(Train_EvalMetrics[[nam]]))
  }
} else {
  print('Train_EvalMetrics is NULL')
}
```

</details>
</p>






##  <font size="5">Variable Importance Table</font>

<details><summary>Variable Importance</summary>
<p>

```{r Model_VarImportanceTable, echo=FALSE}
if(!is.null(All_Importance)) {
  data.table::setorderv(x = All_Importance, cols = names(All_Importance)[2L], order = -1L, na.last = TRUE)
  print(knitr::kable(All_Importance))
} else {
  print("No Importance data was provided")
}
```

</details>
</p>


##  <font size="5">Interaction Importance Table</font>
<p>

<details><summary>Interaction Importance</summary>
<p>

```{r Model_IntImportanceTable, echo=FALSE}
if(!is.null(All_Interaction)) {
  data.table::setorderv(x = All_Interaction, cols = names(All_Interaction)[3L], order = -1L, na.last = TRUE)
  print(knitr::kable(All_Interaction))
} else {
  print('No interaction importance data was provided')
}
```

</details>
</p>



</details>
</p>







# <font size="6">Evaluation Plots</font>
<p>

<details><summary>Expand to view content</summary>
<p>



## <font size="5">Variable Importance Plots</font>
<p>

<details><summary>Expand to view content</summary>
<p>

```{r EvaluationPlots_VIPlot, echo=FALSE}
print('Test Importance')
if(!is.null(Test_Importance)) {
  data.table::setnames(Test_Importance, 'Test_Importance', 'Importance', skip_absent = TRUE)
  eval(RemixAutoML:::VI_Plot(Type = 'catboost', VI_Data = Test_Importance, TopN = 15))
} else {
  print("Test_Importance is NULL")
}
print('Validation Importance')
if(!is.null(Validation_Importance)) {
  data.table::setnames(Validation_Importance, 'Validation_Importance', 'Importance', skip_absent = TRUE)
  eval(RemixAutoML:::VI_Plot(Type = 'catboost', VI_Data = Validation_Importance, TopN = 15))
} else {
  print("Validation_Importance is NULL")
}
print('Train Importance')
if(!is.null(Train_Importance)) {
  data.table::setnames(Train_Importance, 'Train_Importance', 'Importance', skip_absent = TRUE)
  eval(RemixAutoML:::VI_Plot(Type = 'catboost', VI_Data = Train_Importance, TopN = 15))
} else {
  print("Train_Importance is NULL")
}
```

</details>
</p>


## <font size="5">Calibration Plots</font>
<p>

<details><summary>Expand to view content</summary>
<p>

### **TestData**
<p>

<details><summary>Calibration Plot</summary>
<p>

```{r EvaluationPlots_CalibrationPlot, echo=FALSE}
if(!is.null(Test_EvaluationPlot)) {
  eval(Test_EvaluationPlot)
} else {
  print('Test_EvaluationPlot is NULL or TestData is NULL')
}
```

</details>
</p>

### **TrainData** + **ValidationData**
<p>

<details><summary>Calibration Plot</summary>
<p>

```{r EvaluationPlots_CalibrationPlot_Train, echo=FALSE}
if(!is.null(Train_EvaluationPlot)) {
  eval(Train_EvaluationPlot)
} else {
  print('Test_EvaluationPlots is NULL or TrainData is NULL')
}
```

</details>
</p>




## <font size="5">ROC Plots</font>
<p>

<details><summary>Expand to view content</summary>
<p>

### **TestData**
<p>

<details><summary>ROC Plots</summary>
<p>

```{r ROC_Plot, echo=FALSE}
if(!is.null(Test_ROCPlot)) {
  eval(Test_ROCPlot)
} else {
  print('Test_ROCPlot is NULL or TestData is NULL')
}
```

</details>
</p>

### **TrainData** + **ValidationData**
<p>

<details><summary>Expand to view content</summary>
<p>

```{r ROC_Plot_Train, echo=FALSE}
if(!is.null(Train_EvaluationPlot)) {
  eval(Train_ROCPlot)
} else {
  print('Train_ROCPlot is NULL or TrainData is NULL')
}
```

</details>
</p>




## <font size="5">Lift & Gains Plots</font>
<p>

<details><summary>Expand to view content</summary>
<p>

### **TestData**
<p>

<details><summary>Lift & Gains Plots</summary>
<p>

```{r Lift and Gains, echo=FALSE}
if(!is.null(Test_CumGainsChart)) {
  eval(Test_CumGainsChart)
} else {
  print('Test_CumGainsChart is NULL or TestData is NULL')
}
```

</details>
</p>

### **TrainData** + **ValidationData**
<p>

<details><summary>Expand to view content</summary>
<p>

```{r Lift and Gains_Train, echo=FALSE}
if(!is.null(Test_CumGainsChart)) {
  eval(Train_CumGainsChart)
} else {
  print('Train_CumGainsChart is NULL or TestData is NULL')
}
```

</details>
</p>






# <font size="6">Model Interpretation</font>
<p>

<details><summary>Expand to view content</summary>
<p>



## <font size="5">Partial Dependence Plots: Numeric-Features</font> 
<p>

<details><summary>Expand to view content</summary>
<p>


### Partial Dependence Line Plots
<p>

<details><summary>Expand to view content</summary>
<p>


#### **TestData**
<p>

<details><summary>Partital Dependence Line Plots</summary>
<p>

```{r Model_Evaluation_Metrics_NumericVariables, echo=FALSE}
options(warn = -1)
if(!is.null(Test_ParDepPlots) && length(Test_ParDepPlots) > 0) {
  eval(Test_ParDepPlots)
} else {
  print('Test_ParDepPlots is NULL and TestData is NULL')
}
options(warn = 1)
```

</details>
</p>


#### **TrainData** + **ValidationData**
<p>

<details><summary>Partital Dependence Line Plots</summary>
<p>

```{r Model_Evaluation_Metrics_NumericVariables_Train, echo=FALSE}
options(warn = -1)
if(!is.null(Train_ParDepPlots) && length(Train_ParDepPlots) > 0) {
  eval(Train_ParDepPlots)
} else {
  print('Train_ParDepPlots is NULL and TrainData is NULL')
}
options(warn = 1)
```

</details>
</p>



## <font size="5">Partial Partial Dependence Plots: Categorical-Features</font>
<p>

<details><summary>Expand to view content</summary>
<p>


### **TestData**
<p>

<details><summary>Partital Dependence Bar Plots</summary>
<p>

```{r Model_Evaluation_Metrics_CategoricalVariables, echo=FALSE}
options(warn = -1)
if(!is.null(Test_ParDepCatPlots) && length(Test_ParDepCatPlots) > 0) {
  eval(Test_ParDepCatPlots)
} else {
  print('Test_ParDepCatPlots is NULL and TestData is NULL')
}
options(warn = 1)
```

</details>
</p>


### **TrainData** + **ValidationData**
<p>

<details><summary>Partital Dependence Bar Plots</summary>
<p>

```{r Model_Evaluation_Metrics_CategoricalVariables_Train, echo=FALSE}
options(warn = -1)
if(!is.null(Train_ParDepCatPlots) && length(Train_ParDepCatPlots) > 0) {
  eval(Train_ParDepCatPlots)
} else {
  print('Train_ParDepCatPlots is NULL and TrainData is NULL')
}
options(warn = 1)
```

</details>
</p>


</details>
</p>



</details>
</p>






# <font size="6">Model MetaData</font>
<p>

<details><summary>Expand to view content</summary>
<p>


## <font size="5">Parameters and Settings</font> 

<details><summary>Model Parameters</summary>
<p>

```{r Model_MetaData_Parameters, echo=FALSE}
if(!is.null(ArgsList)) {
  for(nam in names(ArgsList)) print(paste0(nam, ": ", ArgsList[[nam]]))
} else {
  txt <- paste0(ModelID, "_ArgsList.Rdata")
  print(paste0('ArgsList is NULL'))
}
```



## <font size="5">Grid Tuning Metrics</font>

<details><summary>Grid Tuning Metrics</summary>
<p>

```{r Model_MetaData_GridMetrics, echo=FALSE}
if(!is.null(GridMetrics)) {
  print(RemixAutoML::DataTable(GridMetrics[order(-MetricValue)]))
} else {
  print("GridTuning was not conducted")
}
```

</details>
</p>



</details>
</p>

