% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/AutoCatBoostCARMA.R
\name{AutoCatBoostCARMA}
\alias{AutoCatBoostCARMA}
\title{AutoCatBoostCARMA}
\usage{
AutoCatBoostCARMA(
  data,
  TimeWeights = NULL,
  NonNegativePred = FALSE,
  RoundPreds = FALSE,
  TrainOnFull = FALSE,
  TargetColumnName = "Target",
  DateColumnName = "DateTime",
  HierarchGroups = NULL,
  GroupVariables = NULL,
  FC_Periods = 30,
  TimeUnit = "week",
  TimeGroups = c("weeks", "months"),
  PDFOutputPath = NULL,
  SaveDataPath = NULL,
  NumOfParDepPlots = 10L,
  TargetTransformation = FALSE,
  Methods = c("BoxCox", "Asinh", "Asin", "Log", "LogPlus1", "Logit", "YeoJohnson"),
  AnomalyDetection = NULL,
  XREGS = NULL,
  Lags = c(1L:5L),
  MA_Periods = c(2L:5L),
  SD_Periods = NULL,
  Skew_Periods = NULL,
  Kurt_Periods = NULL,
  Quantile_Periods = NULL,
  Quantiles_Selected = c("q5", "q95"),
  Difference = TRUE,
  FourierTerms = 6L,
  CalendarVariables = c("second", "minute", "hour", "wday", "mday", "yday", "week",
    "isoweek", "month", "quarter", "year"),
  HolidayVariable = c("USPublicHolidays", "EasterGroup", "ChristmasGroup",
    "OtherEcclesticalFeasts"),
  HolidayLags = 1L,
  HolidayMovingAverages = 1L:2L,
  TimeTrendVariable = FALSE,
  ZeroPadSeries = NULL,
  DataTruncate = FALSE,
  SplitRatios = c(0.7, 0.2, 0.1),
  PartitionType = "timeseries",
  TaskType = "GPU",
  NumGPU = 1,
  EvalMetric = "RMSE",
  EvalMetricValue = 1.5,
  LossFunction = "RMSE",
  LossFunctionValue = 1.5,
  GridTune = FALSE,
  PassInGrid = NULL,
  ModelCount = 100,
  MaxRunsWithoutNewWinner = 50,
  MaxRunMinutes = 24L * 60L,
  Langevin = FALSE,
  DiffusionTemperature = 10000,
  NTrees = 1000,
  L2_Leaf_Reg = 3,
  LearningRate = NULL,
  RandomStrength = 1,
  BorderCount = 254,
  Depth = 6,
  RSM = 1,
  BootStrapType = NULL,
  GrowPolicy = "SymmetricTree",
  Timer = TRUE,
  DebugMode = FALSE
)
}
\arguments{
\item{data}{Supply your full series data set here}

\item{TimeWeights}{Supply a value that will be multiplied by he time trend value}

\item{NonNegativePred}{TRUE or FALSE}

\item{RoundPreds}{Rounding predictions to an integer value. TRUE or FALSE. Defaults to FALSE}

\item{TrainOnFull}{Set to TRUE to train on full data}

\item{TargetColumnName}{List the column name of your target variables column. E.g. "Target"}

\item{DateColumnName}{List the column name of your date column. E.g. "DateTime"}

\item{HierarchGroups}{Vector of hierachy categorical columns.}

\item{GroupVariables}{Defaults to NULL. Use NULL when you have a single series. Add in GroupVariables when you have a series for every level of a group or multiple groups.}

\item{FC_Periods}{Set the number of periods you want to have forecasts for. E.g. 52 for weekly data to forecast a year ahead}

\item{TimeUnit}{List the time unit your data is aggregated by. E.g. "1min", "5min", "10min", "15min", "30min", "hour", "day", "week", "month", "quarter", "year".}

\item{TimeGroups}{Select time aggregations for adding various time aggregated GDL features.}

\item{PDFOutputPath}{NULL or a path file to output PDFs to a specified folder}

\item{SaveDataPath}{NULL Or supply a path. Data saved will be called 'ModelID'_data.csv}

\item{NumOfParDepPlots}{Supply a number for the number of partial dependence plots you want returned}

\item{TargetTransformation}{Run AutoTransformationCreate() to find best transformation for the target variable. Tests YeoJohnson, BoxCox, and Asigh (also Asin and Logit for proportion target variables).}

\item{Methods}{Transformation options to test which include "BoxCox", "Asinh", "Asin", "Log", "LogPlus1", "Logit", "YeoJohnson"}

\item{AnomalyDetection}{NULL for not using the service. Other, provide a list, e.g. AnomalyDetection = list("tstat_high" = 4, tstat_low = -4)}

\item{XREGS}{Additional data to use for model development and forecasting. Data needs to be a complete series which means both the historical and forward looking values over the specified forecast window needs to be supplied.}

\item{Lags}{Select the periods for all lag variables you want to create. E.g. c(1:5,52)}

\item{MA_Periods}{Select the periods for all moving average variables you want to create. E.g. c(1:5,52)}

\item{SD_Periods}{Select the periods for all moving standard deviation variables you want to create. E.g. c(1:5,52)}

\item{Skew_Periods}{Select the periods for all moving skewness variables you want to create. E.g. c(1:5,52)}

\item{Kurt_Periods}{Select the periods for all moving kurtosis variables you want to create. E.g. c(1:5,52)}

\item{Quantile_Periods}{Select the periods for all moving quantiles variables you want to create. E.g. c(1:5,52)}

\item{Quantiles_Selected}{Select from the following "q5", "q10", "q15", "q20", "q25", "q30", "q35", "q40", "q45", "q50", "q55", "q60", "q65", "q70", "q75", "q80", "q85", "q90", "q95"}

\item{Difference}{Puts the I in ARIMA for single series and grouped series.}

\item{FourierTerms}{Set to the max number of pairs. E.g. 2 means to generate two pairs for by each group level and interations if hierarchy is enabled.}

\item{CalendarVariables}{NULL, or select from "second", "minute", "hour", "wday", "mday", "yday", "week", "isoweek", "month", "quarter", "year"}

\item{HolidayVariable}{NULL, or select from "USPublicHolidays", "EasterGroup", "ChristmasGroup", "OtherEcclesticalFeasts"}

\item{HolidayLags}{Number of lags to build off of the holiday count variable.}

\item{HolidayMovingAverages}{Number of moving averages to build off of the holiday count variable.}

\item{TimeTrendVariable}{Set to TRUE to have a time trend variable added to the model. Time trend is numeric variable indicating the numeric value of each record in the time series (by group). Time trend starts at 1 for the earliest point in time and increments by one for each success time point.}

\item{ZeroPadSeries}{NULL to do nothing. Otherwise, set to "maxmax", "minmax", "maxmin", "minmin". See \code{\link{TimeSeriesFill}} for explanations of each type}

\item{DataTruncate}{Set to TRUE to remove records with missing values from the lags and moving average features created}

\item{SplitRatios}{E.g c(0.7,0.2,0.1) for train, validation, and test sets}

\item{PartitionType}{Select "random" for random data partitioning "timeseries" for partitioning by time frames}

\item{TaskType}{Default is "GPU" but you can also set it to "CPU"}

\item{NumGPU}{Defaults to 1. If CPU is set this argument will be ignored.}

\item{EvalMetric}{Select from "RMSE", "MAE", "MAPE", "Poisson", "Quantile", "LogLinQuantile", "Lq", "NumErrors", "SMAPE", "R2", "MSLE", "MedianAbsoluteError"}

\item{EvalMetricValue}{Used when EvalMetric accepts an argument. See \code{\link{AutoCatBoostRegression}}}

\item{LossFunction}{Used in model training for model fitting. Select from 'RMSE', 'MAE', 'Quantile', 'LogLinQuantile', 'MAPE', 'Poisson', 'PairLogitPairwise', 'Tweedie', 'QueryRMSE'}

\item{LossFunctionValue}{Used when LossFunction accepts an argument. See \code{\link{AutoCatBoostRegression}}}

\item{GridTune}{Set to TRUE to run a grid tune}

\item{PassInGrid}{Defaults to NULL}

\item{ModelCount}{Set the number of models to try in the grid tune}

\item{MaxRunsWithoutNewWinner}{Default is 50}

\item{MaxRunMinutes}{Default is 60*60}

\item{Langevin}{Enables the Stochastic Gradient Langevin Boosting mode. If TRUE and TaskType == "GPU" then TaskType will be converted to "CPU"}

\item{DiffusionTemperature}{Default is 10000}

\item{NTrees}{Select the number of trees you want to have built to train the model}

\item{L2_Leaf_Reg}{l2 reg parameter}

\item{LearningRate}{Defaults to NULL. Catboost will dynamically define this if L2_Leaf_Reg is NULL and RMSE is chosen (otherwise catboost will default it to 0.03). Then you can pull it out of the model object and pass it back in should you wish.}

\item{RandomStrength}{Default is 1}

\item{BorderCount}{Default is 254}

\item{Depth}{Depth of catboost model}

\item{RSM}{CPU only. If TaskType is GPU then RSM will not be used}

\item{BootStrapType}{If NULL, then if TaskType is GPU then Bayesian will be used. If CPU then MVS will be used. If MVS is selected when TaskType is GPU, then BootStrapType will be switched to Bayesian}

\item{GrowPolicy}{Default is SymmetricTree. Others include Lossguide and Depthwise}

\item{Timer}{Set to FALSE to turn off the updating print statements for progress}

\item{DebugMode}{Defaults to FALSE. Set to TRUE to get a print statement of each high level comment in function}
}
\value{
Returns a data.table of original series and forecasts, the catboost model objects (everything returned from AutoCatBoostRegression()), a time series forecast plot, and transformation info if you set TargetTransformation to TRUE. The time series forecast plot will plot your single series or aggregate your data to a single series and create a plot from that.
}
\description{
AutoCatBoostCARMA Mutlivariate Forecasting with calendar variables, Holiday counts, holiday lags, holiday moving averages, differencing, transformations, interaction-based categorical encoding using target variable and features to generate various time-based aggregated lags, moving averages, moving standard deviations, moving skewness, moving kurtosis, moving quantiles, parallelized interaction-based fourier pairs by grouping variables, and Trend Variables.
}
\examples{
\dontrun{

# Set up path
Path <- "C:/Users/Bizon/Documents/GitHub"

# Set up environment
data.table::setDTthreads(percent = 100)

# Load data
data <- data.table::fread(file = file.path(Path, "walmart.csv"),index = c("Store","Dept"))

# Set negative numbers to 0
data <- data[, Weekly_Sales := data.table::fifelse(Weekly_Sales < 0, 0, Weekly_Sales)]

# Subset for Stores / Departments with Full Series Available: (143 time points each)----
data <- data[, Counts := .N, by = c("Store","Dept")][Counts == 143][, Counts := NULL]

# Subset Columns (remove IsHoliday column)----
data <- data[, .SD, .SDcols = c("Store","Dept","Date","Weekly_Sales")]

# Setup xregs
xregs <- data[, .SD, .SDcols = c("Date","Store","Dept")]

# Change data types
data[, ":=" (Store = as.character(Store), Dept = as.character(Dept))]
xregs[, ":=" (Store = as.character(Store), Dept = as.character(Dept))]

# Add GroupVar to xregs
xregs[, GroupVar := do.call(paste, c(.SD, sep = " ")), .SDcols = c("Store","Dept")]

# Change names of categoricals in xregs
data.table::setnames(xregs, c("Store","Dept"), c("STORE","DEPT"))

# Subset data so we have an out of time sample
data1 <- data.table::copy(data[, ID := 1:.N, by = c("Store","Dept")][ID <= 125][, ID := NULL])
data[, ID := NULL]

# Define Holdout windows
N <- data1[, .N, by = c("Store","Dept")][1, N]
N1 <- xregs[, .N, by = c("STORE","DEPT")][1, N]

# Setup Grid Tuning & Feature Tuning
Tuning <- data.table::CJ(
  TimeWeights = c("None",0.9999,0.999,0.99),
  HierachGroups = c("TRUE","FALSE"),
  MaxTimeGroups = c("weeks","months","quarters"),
  TargetTransformation = c("TRUE","FALSE"),
  Difference = c("TRUE","FALSE"),
  TimeTrendVariable = c("TRUE","FALSE"),
  EvalMetric = c("RMSE","Huber"),
  LossFunction = c("RMSE","Huber"),
  Langevin = c("TRUE","FALSE"),
  L2_Leaf_Reg = c(1.0,2.0,3.0,4.0))

# Plot list
PlotList <- list()

# Total runs
TotalRuns <- Tuning[,.N]

# Run models
for(Run in seq_len(TotalRuns)) {

  # Print Run
  for(zz in seq_len(100)) print(Run)

  # Use clean data each run
  xregs_new <- data.table::copy(xregs)
  data_new <- data.table::copy(data1)

  # Timer
  StartTime <- Sys.time()

  # Run carma system
  Results <- RemixAutoML::AutoCatBoostCARMA(

    # data args
    data = data_new,
    TimeWeights = if(Tuning[Run, TimeWeights] == "None") NULL else as.numeric(Tuning[Run, TimeWeights]),
    TargetColumnName = "Weekly_Sales",
    DateColumnName = "Date",
    HierarchGroups = if(as.logical(Tuning[Run, HierachGroups])) c("Store","Dept") else NULL,
    GroupVariables = c("Store","Dept"),
    TimeUnit = "weeks",
    TimeGroups = if(Tuning[Run, MaxTimeGroups] == "weeks") "weeks" else if(Tuning[Run, MaxTimeGroups] == "months") c("weeks","months") else c("weeks","months","quarters"),

    # Production args
    TrainOnFull = TRUE,
    SplitRatios = c(N / N1, 1 - N / N1),
    PartitionType = "random",
    FC_Periods = N1-N,
    TaskType = "GPU",
    NumGPU = 1,
    Timer = TRUE,
    DebugMode = TRUE,

    # Target transformations
    TargetTransformation = as.logical(Tuning[Run, TargetTransformation]),
    Methods = c("BoxCox","Asinh","Log","LogPlus1","YeoJohnson"),
    Difference = as.logical(Tuning[Run, Difference]),
    NonNegativePred = TRUE,
    RoundPreds = as.logical(Tuning[Run, RoundPreds]),

    # Calendar features
    CalendarVariables = c("week","wom","month","quarter"),
    HolidayVariable = c("USPublicHolidays","EasterGroup","ChristmasGroup","OtherEcclesticalFeasts"),
    HolidayLags = c(1,2,3),
    HolidayMovingAverages = c(2,3),

    # Time series features
    Lags = list("weeks" = c(1,2,3,4,5,8,9,12,13,51,52,53), "months" = c(1,2,6,12)),
    MA_Periods = list("weeks" = c(2,3,4,5,8,9,12,13,51,52,53), "months" = c(2,6,12)),
    SD_Periods = NULL,
    Skew_Periods = NULL,
    Kurt_Periods = NULL,
    Quantile_Periods = NULL,
    Quantiles_Selected = NULL,

    # Bonus features
    AnomalyDetection = NULL,
    XREGS = xregs_new,
    FourierTerms = 0,
    TimeTrendVariable = as.logical(Tuning[Run, TimeTrendVariable]),
    ZeroPadSeries = NULL,
    DataTruncate = FALSE,

    # ML evaluation output
    PDFOutputPath = NULL,
    SaveDataPath = NULL,
    NumOfParDepPlots = 0L,

    # ML loss functions
    EvalMetric = Tuning[Run, EvalMetric],
    EvalMetricValue = 10,
    LossFunction = Tuning[Run, LossFunction],
    LossFunctionValue = 10,

    # ML grid tuning args
    GridTune = FALSE,
    PassInGrid = NULL,
    ModelCount = 5,
    MaxRunsWithoutNewWinner = 50,
    MaxRunMinutes = 60*60,

    # ML tuning args
    NTrees = 12000,
    Depth = 9,
    L2_Leaf_Reg = Tuning[Run, L2_Leaf_Reg],
    LearningRate = NULL,
    Langevin = as.logical(Tuning[Run, Langevin]),
    DiffusionTemperature = 10000,
    RandomStrength = 1,
    BorderCount = 254,
    BootStrapType = c("Bayesian","Bernoulli","Poisson","MVS","No"))

  # Timer
  EndTime <- Sys.time()

  # Prepare data for evaluation
  Results <- Results$Forecast
  data.table::setnames(Results, "Weekly_Sales", "Old")
  Results <- merge(Results, data, by = c("Store","Dept","Date"), all = FALSE)
  Results <- Results[is.na(Old)]
  Results[, Old := NULL]

  # Create totals and subtotals
  Results <- data.table::groupingsets(
    x = Results,
    j = list(Predictions = sum(Predictions), Weekly_Sales = sum(Weekly_Sales)),
    by = c("Date", "Store", "Dept"),
    sets = list(c("Date", "Store", "Dept"), c("Store", "Dept"), "Store", "Dept", "Date"))
  Results[, Store := data.table::fifelse(is.na(Store), "Total", Store)]
  Results[, Dept := data.table::fifelse(is.na(Dept), "Total", Dept)]

  # Add error measures
  Results[, Weekly_MAE := abs(Weekly_Sales - Predictions)]
  Results[, Weekly_MAPE := Weekly_MAE / Weekly_Sales]

  # Weekly results
  Weekly_MAPE <- Results[, list(Weekly_MAPE = mean(Weekly_MAPE)), by = list(Store,Dept)]

  # Monthly results
  temp <- data.table::copy(Results)
  temp <- temp[, Date := lubridate::floor_date(Date, unit = "months")]
  temp <- temp[, lapply(.SD, sum), by = c("Date","Store","Dept"), .SDcols = c("Predictions", "Weekly_Sales")]
  temp[, Monthly_MAE := abs(Weekly_Sales - Predictions)]
  temp[, Monthly_MAPE := Monthly_MAE / Weekly_Sales]
  Monthly_MAPE <- temp[, list(Monthly_MAPE = mean(Monthly_MAPE)), by = list(Store,Dept)]

  # Create ts plot of actuals and predicted
  Totals <- Results[Store == "Total" & Dept == "Total"]
  Totals <- data.table::melt.data.table(data = Totals, id.vars = "Date", measure.vars = c("Predictions","Weekly_Sales"), variable.name = "Series", value.name = "Weekly_Sales")
  PlotList[[Run]] <- eval(ggplot2::ggplot(data = Totals, ggplot2::aes(x = Date, y = Weekly_Sales, color = Series)) +
                            ggplot2::geom_line() +
                            ggplot2::scale_color_manual(values = c("red","blue")) +
                            ggplot2::labs(
                              title = "Walmart Data Forecast",
                              subtitle = paste0("Weekly MAPE = ", round(100 * Weekly_MAPE[Store == "Total" & Dept == "Total", Weekly_MAPE],1),"\%", " :: Monthly MAPE = ", round(100 * Monthly_MAPE[Store == "Total" & Dept == "Total", Monthly_MAPE],1),"\%")) +
                            RemixAutoML::ChartTheme(Size = 10, AngleX = 0, AngleY = 0))

  # Collect metrics
  Metrics <- data.table::data.table(
    RunNumber = Run,
    Total_Weekly_MAPE = Weekly_MAPE[Store == "Total" & Dept == "Total", Weekly_MAPE],
    Total_Monthly_MAPE = Monthly_MAPE[Store == "Total" & Dept == "Total", Monthly_MAPE],
    Tuning[Run],
    RunTime = EndTime - StartTime)

  # Append to file
  data.table::fwrite(Metrics, file = file.path(Path, "Walmart_CARMA_Metrics.csv"), append = TRUE)
}
}
}
\seealso{
Other Automated Panel Data Forecasting: 
\code{\link{AutoCatBoostHurdleCARMA}()},
\code{\link{AutoCatBoostVectorCARMA}()},
\code{\link{AutoH2OCARMA}()},
\code{\link{AutoXGBoostCARMA}()}
}
\author{
Adrian Antico
}
\concept{Automated Panel Data Forecasting}
