% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/AutoXGBoostHurdleModel.R
\name{AutoXGBoostHurdleModel}
\alias{AutoXGBoostHurdleModel}
\title{AutoXGBoostHurdleModel}
\usage{
AutoXGBoostHurdleModel(
  TreeMethod = "hist",
  TrainOnFull = FALSE,
  PassInGrid = NULL,
  NThreads = max(1L, parallel::detectCores() - 2L),
  ModelID = "ModelTest",
  Paths = NULL,
  MetaDataPaths = NULL,
  data,
  ValidationData = NULL,
  TestData = NULL,
  Buckets = 0L,
  TargetColumnName = NULL,
  FeatureColNames = NULL,
  PrimaryDateColumn = NULL,
  WeightsColumnName = NULL,
  ClassWeights = c(1, 1),
  IDcols = NULL,
  DebugMode = FALSE,
  EncodingMethod = "credibility",
  TransformNumericColumns = NULL,
  Methods = c("Asinh", "Asin", "Log", "LogPlus1", "Sqrt", "Logit"),
  SplitRatios = c(0.7, 0.2, 0.1),
  SaveModelObjects = FALSE,
  ReturnModelObjects = TRUE,
  NumOfParDepPlots = 1L,
  GridTune = FALSE,
  grid_eval_metric = "accuracy",
  MaxModelsInGrid = 1L,
  BaselineComparison = "default",
  MaxRunsWithoutNewWinner = 10L,
  MaxRunMinutes = 60L,
  Trees = list(classifier = 1000, regression = 1000),
  eta = list(classifier = 0.05, regression = 0.05),
  max_depth = list(classifier = 4L, regression = 4L),
  min_child_weight = list(classifier = 1, regression = 1),
  subsample = list(classifier = 0.55, regression = 0.55),
  colsample_bytree = list(classifier = 0.55, regression = 0.55)
)
}
\arguments{
\item{TreeMethod}{Set to hist or gpu_hist depending on if you have an xgboost installation capable of gpu processing}

\item{TrainOnFull}{Set to TRUE to train model on 100 percent of data}

\item{PassInGrid}{Pass in a grid for changing up the parameter settings for catboost}

\item{NThreads}{Set to the number of threads you would like to dedicate to training}

\item{ModelID}{Define a character name for your models}

\item{Paths}{The path to your folder where you want your model information saved}

\item{MetaDataPaths}{A character string of your path file to where you want your model evaluation output saved. If left NULL, all output will be saved to Paths.}

\item{data}{Source training data. Do not include a column that has the class labels for the buckets as they are created internally.}

\item{ValidationData}{Source validation data. Do not include a column that has the class labels for the buckets as they are created internally.}

\item{TestData}{Souce test data. Do not include a column that has the class labels for the buckets as they are created internally.}

\item{Buckets}{A numeric vector of the buckets used for subsetting the data. NOTE: the final Bucket value will first create a subset of data that is less than the value and a second one thereafter for data greater than the bucket value.}

\item{TargetColumnName}{Supply the column name or number for the target variable}

\item{FeatureColNames}{Supply the column names or number of the features (not included the PrimaryDateColumn)}

\item{PrimaryDateColumn}{Date column for sorting}

\item{WeightsColumnName}{Weighs column name}

\item{ClassWeights}{Look up the classifier model help file}

\item{IDcols}{Includes PrimaryDateColumn and any other columns you want returned in the validation data with predictions}

\item{DebugMode}{For debugging}

\item{EncodingMethod}{Choose from 'binary', 'poly_encode', 'backward_difference', 'helmert' for multiclass cases and additionally 'm_estimator', 'credibility', 'woe', 'target_encoding' for classification use cases.}

\item{TransformNumericColumns}{Transform numeric column inside the AutoCatBoostRegression() function}

\item{Methods}{Choose from 'Asinh', 'Asin', 'Log', 'LogPlus1', 'Sqrt', 'Logit'}

\item{SplitRatios}{Supply vector of partition ratios. For example, c(0.70,0.20,0,10)}

\item{SaveModelObjects}{Set to TRUE to save the model objects to file in the folders listed in Paths}

\item{ReturnModelObjects}{Set to TRUE to return all model objects}

\item{NumOfParDepPlots}{Set to pull back N number of partial dependence calibration plots.}

\item{GridTune}{Set to TRUE if you want to grid tune the models}

\item{grid_eval_metric}{Select the metric to optimize in grid tuning. "accuracy", "microauc", "logloss"}

\item{MaxModelsInGrid}{Set to a numeric value for the number of models to try in grid tune}

\item{BaselineComparison}{"default"}

\item{MaxRunsWithoutNewWinner}{Number of runs without a new winner before stopping the grid tuning}

\item{MaxRunMinutes}{Max number of minutes to allow the grid tuning to run for}

\item{Trees}{Provide a named list to have different number of trees for each model. Trees = list("classifier" = seq(1000,2000,100), "regression" = seq(1000,2000,100))}

\item{eta}{Provide a named list to have different number of eta for each model.}

\item{max_depth}{Provide a named list to have different number of max_depth for each model.}

\item{min_child_weight}{Provide a named list to have different number of min_child_weight for each model.}

\item{subsample}{Provide a named list to have different number of subsample for each model.}

\item{colsample_bytree}{Provide a named list to have different number of colsample_bytree for each model.}
}
\description{
AutoXGBoostHurdleModel is generalized hurdle modeling framework
}
\examples{
\dontrun{
# Test data.table
XGBoost_QA <- data.table::CJ(
  TOF = c(TRUE,FALSE),
  Classification = c(TRUE,FALSE),
  Success = "Failure",
  ScoreSuccess = "Failure",
  PartitionInFunction = c(TRUE,FALSE), sorted = FALSE
)

# Remove impossible combinations
XGBoost_QA <- XGBoost_QA[!(PartitionInFunction & TOF)]
XGBoost_QA[, RunNumber := seq_len(.N)]

# Path File
Path <- getwd()

#      TOF Classification Success PartitionInFunction RunNumber
# 1:  TRUE           TRUE Failure               FALSE         1
# 2:  TRUE          FALSE Failure               FALSE         2
# 3: FALSE           TRUE Failure                TRUE         3
# 4: FALSE           TRUE Failure               FALSE         4
# 5: FALSE          FALSE Failure                TRUE         5
# 6: FALSE          FALSE Failure               FALSE         6

# AutoCatBoostHurdleModel
# run = 5
# run = 6
for(run in seq_len(XGBoost_QA[,.N])) {

  # Define values
  tof <- XGBoost_QA[run, TOF]
  PartitionInFunction <- XGBoost_QA[run, PartitionInFunction]
  Classify <- XGBoost_QA[run, Classification]
  Tar <- "Adrian"

  # Get data
  if(Classify) {
    data <- RemixAutoML::FakeDataGenerator(N = 15000, ZIP = 1)
  } else {
    data <- RemixAutoML::FakeDataGenerator(N = 100000, ZIP = 2)
  }

  # Partition Data
  if(!tof && !PartitionInFunction) {
    Sets <- RemixAutoML::AutoDataPartition(
      data = data,
      NumDataSets = 3,
      Ratios = c(0.7,0.2,0.1),
      PartitionType = "random",
      StratifyColumnNames = "Adrian",
      TimeColumnName = NULL)
    TTrainData <- Sets$TrainData
    VValidationData <- Sets$ValidationData
    TTestData <- Sets$TestData
    rm(Sets)
  } else {
    TTrainData <- data.table::copy(data)
    VValidationData <- NULL
    TTestData <- NULL
  }

  # Run function
  TestModel <- tryCatch({RemixAutoML::AutoXGBoostHurdleModel(

    # Operationalization
    ModelID = 'ModelTest',
    SaveModelObjects = FALSE,
    ReturnModelObjects = TRUE,
    NThreads = parallel::detectCores(),

    # Data related args
    data = TTrainData,
    ValidationData = VValidationData,
    PrimaryDateColumn = "DateTime",
    TestData = TTestData,
    WeightsColumnName = NULL,
    TrainOnFull = tof,
    Buckets = if(Classify) 0L else c(0,2,3),
    TargetColumnName = "Adrian",
    FeatureColNames = names(TTrainData)[!names(data) \%in\% c("Adrian","IDcol_1","IDcol_2","IDcol_3","IDcol_4","IDcol_5","DateTime")],
    IDcols = c("IDcol_1","IDcol_2","IDcol_3","IDcol_4","IDcol_5","DateTime"),
    DebugMode = TRUE,

    # Metadata args
    EncodingMethod = "credibility",
    Paths = normalizePath('./'),
    MetaDataPaths = NULL,
    TransformNumericColumns = NULL,
    Methods = c('Asinh', 'Asin', 'Log', 'LogPlus1', 'Logit'),
    ClassWeights = c(1,1),
    SplitRatios = if(PartitionInFunction) c(0.70, 0.20, 0.10) else NULL,
    NumOfParDepPlots = 10L,

    # Grid tuning setup
    PassInGrid = NULL,
    GridTune = FALSE,
    BaselineComparison = 'default',
    MaxModelsInGrid = 1L,
    MaxRunsWithoutNewWinner = 20L,
    MaxRunMinutes = 60L*60L,

    # XGBoost parameters
    TreeMethod = "hist",
    Trees = list("classifier" = 50, "regression" = 50),
    eta = list("classifier" = 0.05, "regression" = 0.05),
    max_depth = list("classifier" = 4L, "regression" = 4L),
    min_child_weight = list("classifier" = 1.0, "regression" = 1.0),
    subsample = list("classifier" = 0.55, "regression" = 0.55),
    colsample_bytree = list("classifier" = 0.55, "regression" = 0.55))}, error = function(x) NULL)

  # Outcome
  if(!is.null(TestModel)) XGBoost_QA[run, Success := "Success"]
  data.table::fwrite(XGBoost_QA, file = "C:/Users/Bizon/Documents/GitHub/QA_Code/QA_CSV/AutoXGBoostHurdleModel_QA.csv")

  # Remove Target Variable
  TTrainData[, c("Target_Buckets", "Adrian") := NULL]

  # Score XGBoost Hurdle Model
  Output <- tryCatch({RemixAutoML::AutoXGBoostHurdleModelScoring(
    TestData = TTrainData,
    Path = Path,
    ModelID = "ModelTest",
    ModelList = TestModel$ModelList,
    ArgsList = TestModel$ArgsList,
    Threshold = NULL)}, error = function(x) NULL)

  # Outcome
  if(!is.null(Output)) XGBoost_QA[run, Score := "Success"]
  TestModel <- NULL
  Output <- NULL
  TTrainData <- NULL
  VValidationData <- NULL
  TTestData <- NULL
  gc(); Sys.sleep(5)
  data.table::fwrite(XGBoost_QA, file = file.path(Path, "AutoXGBoostHurdleModel_QA.csv"))
}
}
}
\seealso{
Other Supervised Learning - Hurdle Modeling: 
\code{\link{AutoCatBoostHurdleModel}()},
\code{\link{AutoH2oDRFHurdleModel}()},
\code{\link{AutoH2oGBMHurdleModel}()},
\code{\link{AutoLightGBMHurdleModel}()}
}
\author{
Adrian Antico
}
\concept{Supervised Learning - Hurdle Modeling}
