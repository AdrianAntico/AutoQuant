% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/AutoXGBoostHurdleModel.R
\name{AutoXGBoostHurdleModel}
\alias{AutoXGBoostHurdleModel}
\title{AutoXGBoostHurdleModel}
\usage{
AutoXGBoostHurdleModel(
  TreeMethod = "hist",
  TrainOnFull = FALSE,
  PassInGrid = NULL,
  NThreads = max(1L, parallel::detectCores() - 2L),
  ModelID = "ModelTest",
  Paths = NULL,
  MetaDataPaths = NULL,
  data,
  ValidationData = NULL,
  TestData = NULL,
  Buckets = 0L,
  TargetColumnName = NULL,
  FeatureColNames = NULL,
  PrimaryDateColumn = NULL,
  WeightsColumnName = NULL,
  ClassWeights = c(1, 1),
  IDcols = NULL,
  DebugMode = FALSE,
  EncodingMethod = "credibility",
  TransformNumericColumns = NULL,
  Methods = c("Asinh", "Asin", "Log", "LogPlus1", "Sqrt", "Logit"),
  SplitRatios = c(0.7, 0.2, 0.1),
  SaveModelObjects = FALSE,
  ReturnModelObjects = TRUE,
  NumOfParDepPlots = 1L,
  GridTune = FALSE,
  grid_eval_metric = "accuracy",
  MaxModelsInGrid = 1L,
  BaselineComparison = "default",
  MaxRunsWithoutNewWinner = 10L,
  MaxRunMinutes = 60L,
  Trees = list(classifier = 1000, regression = 1000),
  eta = list(classifier = 0.05, regression = 0.05),
  max_depth = list(classifier = 4L, regression = 4L),
  min_child_weight = list(classifier = 1, regression = 1),
  subsample = list(classifier = 0.55, regression = 0.55),
  colsample_bytree = list(classifier = 0.55, regression = 0.55)
)
}
\arguments{
\item{TreeMethod}{Set to hist or gpu_hist depending on if you have an xgboost installation capable of gpu processing}

\item{TrainOnFull}{Set to TRUE to train model on 100 percent of data}

\item{PassInGrid}{Pass in a grid for changing up the parameter settings for catboost}

\item{NThreads}{Set to the number of threads you would like to dedicate to training}

\item{ModelID}{Define a character name for your models}

\item{Paths}{The path to your folder where you want your model information saved}

\item{MetaDataPaths}{A character string of your path file to where you want your model evaluation output saved. If left NULL, all output will be saved to Paths.}

\item{data}{Source training data. Do not include a column that has the class labels for the buckets as they are created internally.}

\item{ValidationData}{Source validation data. Do not include a column that has the class labels for the buckets as they are created internally.}

\item{TestData}{Souce test data. Do not include a column that has the class labels for the buckets as they are created internally.}

\item{Buckets}{A numeric vector of the buckets used for subsetting the data. NOTE: the final Bucket value will first create a subset of data that is less than the value and a second one thereafter for data greater than the bucket value.}

\item{TargetColumnName}{Supply the column name or number for the target variable}

\item{FeatureColNames}{Supply the column names or number of the features (not included the PrimaryDateColumn)}

\item{PrimaryDateColumn}{Date column for sorting}

\item{WeightsColumnName}{Weighs column name}

\item{ClassWeights}{Look up the classifier model help file}

\item{IDcols}{Includes PrimaryDateColumn and any other columns you want returned in the validation data with predictions}

\item{DebugMode}{For debugging}

\item{EncodingMethod}{Choose from 'binary', 'poly_encode', 'backward_difference', 'helmert' for multiclass cases and additionally 'm_estimator', 'credibility', 'woe', 'target_encoding' for classification use cases.}

\item{TransformNumericColumns}{Transform numeric column inside the AutoCatBoostRegression() function}

\item{Methods}{Choose from 'Asinh', 'Asin', 'Log', 'LogPlus1', 'Sqrt', 'Logit'}

\item{SplitRatios}{Supply vector of partition ratios. For example, c(0.70,0.20,0,10)}

\item{SaveModelObjects}{Set to TRUE to save the model objects to file in the folders listed in Paths}

\item{ReturnModelObjects}{Set to TRUE to return all model objects}

\item{NumOfParDepPlots}{Set to pull back N number of partial dependence calibration plots.}

\item{GridTune}{Set to TRUE if you want to grid tune the models}

\item{grid_eval_metric}{Select the metric to optimize in grid tuning. "accuracy", "microauc", "logloss"}

\item{MaxModelsInGrid}{Set to a numeric value for the number of models to try in grid tune}

\item{BaselineComparison}{"default"}

\item{MaxRunsWithoutNewWinner}{Number of runs without a new winner before stopping the grid tuning}

\item{MaxRunMinutes}{Max number of minutes to allow the grid tuning to run for}

\item{Trees}{Provide a named list to have different number of trees for each model. Trees = list("classifier" = seq(1000,2000,100), "regression" = seq(1000,2000,100))}

\item{eta}{Provide a named list to have different number of eta for each model.}

\item{max_depth}{Provide a named list to have different number of max_depth for each model.}

\item{min_child_weight}{Provide a named list to have different number of min_child_weight for each model.}

\item{subsample}{Provide a named list to have different number of subsample for each model.}

\item{colsample_bytree}{Provide a named list to have different number of colsample_bytree for each model.}
}
\description{
AutoXGBoostHurdleModel is generalized hurdle modeling framework
}
\examples{
\dontrun{
Output <- RemixAutoML::AutoXGBoostHurdleModel(

   # Operationalization args
   TrainOnFull = FALSE,
   PassInGrid = NULL,

   # Metadata args
   NThreads = max(1L, parallel::detectCores()-2L),
   ModelID = "ModelTest",
   Paths = normalizePath("./"),
   MetaDataPaths = NULL,

   # data args
   data,
   ValidationData = NULL,
   TestData = NULL,
   Buckets = 0L,
   TargetColumnName = NULL,
   FeatureColNames = NULL,
   PrimaryDateColumn = NULL,
   WeightsColumnName = NULL,
   IDcols = NULL,
   ClassWeights = c(1,1),
   DebugMode = FALSE,

   # options
   EncodingMethod = "credibility",
   TransformNumericColumns = NULL,
   Methods = c('Asinh','Asin','Log','LogPlus1','Sqrt','Logit'),
   SplitRatios = c(0.70, 0.20, 0.10),
   ReturnModelObjects = TRUE,
   SaveModelObjects = FALSE,
   NumOfParDepPlots = 10L,

   # grid tuning args
   GridTune = FALSE,
   grid_eval_metric = "accuracy",
   MaxModelsInGrid = 1L,
   BaselineComparison = "default",
   MaxRunsWithoutNewWinner = 10L,
   MaxRunMinutes = 60L,

   # XGBoost parameters
   TreeMethod = "hist",
   Trees = list("classifier" = 1000, "regression" = 1000),
   eta = list("classifier" = 0.05, "regression" = 0.05),
   max_depth = list("classifier" = 4L, "regression" = 4L),
   min_child_weight = list("classifier" = 1.0, "regression" = 1.0),
   subsample = list("classifier" = 0.55, "regression" = 0.55),
   colsample_bytree = list("classifier" = 0.55, "regression" = 0.55))
}
}
\seealso{
Other Supervised Learning - Hurdle Modeling: 
\code{\link{AutoCatBoostHurdleModel}()},
\code{\link{AutoH2oDRFHurdleModel}()},
\code{\link{AutoH2oGBMHurdleModel}()},
\code{\link{AutoLightGBMHurdleModel}()}
}
\author{
Adrian Antico
}
\concept{Supervised Learning - Hurdle Modeling}
