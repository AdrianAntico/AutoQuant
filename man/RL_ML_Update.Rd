% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ReinforcementLearningFunctions.R
\name{RL_ML_Update}
\alias{RL_ML_Update}
\title{RL_ML_Update}
\usage{
RL_ML_Update(
  ModelType = "classification",
  Iteration = counter,
  NewGrid. = NewGrid,
  NewPerformance. = NewPerformance,
  BestPerformance. = BestPerformance,
  Trials. = Trials,
  Successes. = Successes,
  GridIDs. = GridIDs,
  BanditArmsN. = BanditArmsN,
  RunsWithoutNewWinner. = RunsWithoutNewWinner,
  MaxRunsWithoutNewWinner. = MaxRunsWithoutNewWinner,
  MaxModelsInGrid. = MaxModelsInGrid,
  MaxRunMinutes. = MaxRunMinutes,
  TotalRunTime. = TotalRunTime,
  BanditProbs. = BanditProbs
)
}
\arguments{
\item{ModelType}{"classification", "regression", and "multiclass"}

\item{Iteration}{Model iteration number}

\item{NewGrid.}{Previous grid passed in}

\item{NewPerformance.}{Internal}

\item{BestPerformance.}{Internal}

\item{Trials.}{Numeric vector with the total trials for each arm}

\item{Successes.}{Numeric vector with the total successes for each arm}

\item{GridIDs.}{The numeric vector that identifies which grid is which}

\item{BanditArmsN.}{The number of arms in the bandit}

\item{RunsWithoutNewWinner.}{Counter of the number of models previously built without being a new winner}

\item{MaxRunsWithoutNewWinner.}{Maximum number of models built without a new best model (constraint)}

\item{MaxModelsInGrid.}{Maximum number of models to build (constraint)}

\item{MaxRunMinutes.}{Run time constraint}

\item{TotalRunTime.}{Cumulative run time in minutes}

\item{BanditProbs.}{Inital probabilities from RL_Initialize()}
}
\description{
RL_ML_Update updates the bandit probabilities for selecting different grids
}
\examples{
\dontrun{
RL_Update_Output <- RL_ML_Update(
  ModelType = "classification",
  Iteration = run,
  NewGrid. = NewGrid,
  NewPerformance. = NewPerformance,
  BestPerformance. = BestPerformance,
  Trials. = Trials,
  Successes. = Successes,
  GridIDs. = GridIDs,
  BanditArmsN. = BanditArmsN,
  RunsWithoutNewWinner. = RunsWithoutNewWinner,
  MaxRunsWithoutNewWinner. = MaxRunsWithoutNewWinner,
  MaxNumberModels. = MaxNumberModels,
  MaxRunMinutes. = MaxRunMinutes,
  TotalRunTime. = TotalRunTime,
  BanditProbs. = BanditProbs)
BanditProbs <- RL_Update_Output[["BanditProbs"]]
Trials <- RL_Update_Output[["Trials"]]
Successes <- RL_Update_Output[["Successes"]]
NewGrid <- RL_Update_Output[["NewGrid"]]
}
}
\seealso{
Other Reinforcement Learning: 
\code{\link{GridTuner}()},
\code{\link{RL_Initialize}()},
\code{\link{RL_Update}()},
\code{\link{RPM_Binomial_Bandit}()}
}
\author{
Adrian Antico
}
\concept{Reinforcement Learning}
