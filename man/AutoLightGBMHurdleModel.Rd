% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/AutoLightGBMHurdleModel.R
\name{AutoLightGBMHurdleModel}
\alias{AutoLightGBMHurdleModel}
\title{AutoLightGBMHurdleModel}
\usage{
AutoLightGBMHurdleModel(
  TrainOnFull = FALSE,
  PassInGrid = NULL,
  NThreads = max(1L, parallel::detectCores() - 2L),
  ModelID = "ModelTest",
  Paths = NULL,
  MetaDataPaths = NULL,
  data,
  ValidationData = NULL,
  TestData = NULL,
  Buckets = 0L,
  TargetColumnName = NULL,
  FeatureColNames = NULL,
  PrimaryDateColumn = NULL,
  WeightsColumnName = NULL,
  ClassWeights = c(1, 1),
  IDcols = NULL,
  DebugMode = FALSE,
  EncodingMethod = "credibility",
  TransformNumericColumns = NULL,
  Methods = c("Asinh", "Asin", "Log", "LogPlus1", "Sqrt", "Logit"),
  SplitRatios = c(0.7, 0.2, 0.1),
  SaveModelObjects = FALSE,
  ReturnModelObjects = TRUE,
  NumOfParDepPlots = 1L,
  GridTune = FALSE,
  grid_eval_metric = "accuracy",
  MaxModelsInGrid = 1L,
  BaselineComparison = "default",
  MaxRunsWithoutNewWinner = 10L,
  MaxRunMinutes = 60L,
  input_model = list(classifier = NULL, regression = NULL),
  task = list(classifier = "train", regression = "train"),
  device_type = list(classifier = "CPU", regression = "CPU"),
  objective = list(classifier = "binary", regression = "regression"),
  metric = list(classifier = "binary_logloss", regression = "rmse"),
  boosting = list(classifier = "gbdt", regression = "gbdt"),
  LinearTree = list(classifier = FALSE, regression = FALSE),
  Trees = list(classifier = 1000L, regression = 1000L),
  eta = list(classifier = NULL, regression = NULL),
  num_leaves = list(classifier = 31, regression = 31),
  deterministic = list(classifier = TRUE, regression = TRUE),
  force_col_wise = list(classifier = FALSE, regression = FALSE),
  force_row_wise = list(classifier = FALSE, regression = FALSE),
  max_depth = list(classifier = NULL, regression = NULL),
  min_data_in_leaf = list(classifier = 20, regression = 20),
  min_sum_hessian_in_leaf = list(classifier = 0.001, regression = 0.001),
  bagging_freq = list(classifier = 0, regression = 0),
  bagging_fraction = list(classifier = 1, regression = 1),
  feature_fraction = list(classifier = 1, regression = 1),
  feature_fraction_bynode = list(classifier = 1, regression = 1),
  extra_trees = list(classifier = FALSE, regression = FALSE),
  early_stopping_round = list(classifier = 10, regression = 10),
  first_metric_only = list(classifier = TRUE, regression = TRUE),
  max_delta_step = list(classifier = 0, regression = 0),
  lambda_l1 = list(classifier = 0, regression = 0),
  lambda_l2 = list(classifier = 0, regression = 0),
  linear_lambda = list(classifier = 0, regression = 0),
  min_gain_to_split = list(classifier = 0, regression = 0),
  drop_rate_dart = list(classifier = 0.1, regression = 0.1),
  max_drop_dart = list(classifier = 50, regression = 50),
  skip_drop_dart = list(classifier = 0.5, regression = 0.5),
  uniform_drop_dart = list(classifier = FALSE, regression = FALSE),
  top_rate_goss = list(classifier = FALSE, regression = FALSE),
  other_rate_goss = list(classifier = FALSE, regression = FALSE),
  monotone_constraints = list(classifier = NULL, regression = NULL),
  monotone_constraints_method = list(classifier = "advanced", regression = "advanced"),
  monotone_penalty = list(classifier = 0, regression = 0),
  forcedsplits_filename = list(classifier = NULL, regression = NULL),
  refit_decay_rate = list(classifier = 0.9, regression = 0.9),
  path_smooth = list(classifier = 0, regression = 0),
  max_bin = list(classifier = 255, regression = 255),
  min_data_in_bin = list(classifier = 3, regression = 3),
  data_random_seed = list(classifier = 1, regression = 1),
  is_enable_sparse = list(classifier = TRUE, regression = TRUE),
  enable_bundle = list(classifier = TRUE, regression = TRUE),
  use_missing = list(classifier = TRUE, regression = TRUE),
  zero_as_missing = list(classifier = FALSE, regression = FALSE),
  two_round = list(classifier = FALSE, regression = FALSE),
  convert_model = list(classifier = NULL, regression = NULL),
  convert_model_language = list(classifier = "cpp", regression = "cpp"),
  boost_from_average = list(classifier = TRUE, regression = TRUE),
  is_unbalance = list(classifier = FALSE, regression = FALSE),
  scale_pos_weight = list(classifier = 1, regression = 1),
  is_provide_training_metric = list(classifier = TRUE, regression = TRUE),
  eval_at = list(classifier = c(1, 2, 3, 4, 5), regression = c(1, 2, 3, 4, 5)),
  num_machines = list(classifier = 1, regression = 1),
  gpu_platform_id = list(classifier = -1, regression = -1),
  gpu_device_id = list(classifier = -1, regression = -1),
  gpu_use_dp = list(classifier = TRUE, regression = TRUE),
  num_gpu = list(classifier = 1, regression = 1)
)
}
\arguments{
\item{TrainOnFull}{Set to TRUE to train model on 100 percent of data}

\item{PassInGrid}{Pass in a grid for changing up the parameter settings for catboost

# Core parameters https://lightgbm.readthedocs.io/en/latest/Parameters.html#core-parameter}

\item{NThreads}{only list up to number of cores, not threads. parallel::detectCores() / 2}

\item{ModelID}{Define a character name for your models}

\item{Paths}{The path to your folder where you want your model information saved}

\item{MetaDataPaths}{A character string of your path file to where you want your model evaluation output saved. If left NULL, all output will be saved to Paths.}

\item{data}{Source training data. Do not include a column that has the class labels for the buckets as they are created internally.}

\item{ValidationData}{Source validation data. Do not include a column that has the class labels for the buckets as they are created internally.}

\item{TestData}{Souce test data. Do not include a column that has the class labels for the buckets as they are created internally.}

\item{Buckets}{A numeric vector of the buckets used for subsetting the data. NOTE: the final Bucket value will first create a subset of data that is less than the value and a second one thereafter for data greater than the bucket value.}

\item{TargetColumnName}{Supply the column name or number for the target variable}

\item{FeatureColNames}{Supply the column names or number of the features (not included the PrimaryDateColumn)}

\item{PrimaryDateColumn}{Date column for sorting}

\item{WeightsColumnName}{Weighs column name}

\item{ClassWeights}{Look up the classifier model help file}

\item{IDcols}{Includes PrimaryDateColumn and any other columns you want returned in the validation data with predictions}

\item{DebugMode}{For debugging}

\item{EncodingMethod}{Choose from 'binary', 'poly_encode', 'backward_difference', 'helmert' for multiclass cases and additionally 'm_estimator', 'credibility', 'woe', 'target_encoding' for classification use cases.}

\item{TransformNumericColumns}{Transform numeric column inside the AutoCatBoostRegression() function}

\item{Methods}{Choose from 'Asinh', 'Asin', 'Log', 'LogPlus1', 'Sqrt', 'Logit'}

\item{SplitRatios}{Supply vector of partition ratios. For example, c(0.70,0.20,0,10)}

\item{SaveModelObjects}{Set to TRUE to save the model objects to file in the folders listed in Paths}

\item{ReturnModelObjects}{Set to TRUE to return all model objects}

\item{NumOfParDepPlots}{Set to pull back N number of partial dependence calibration plots.}

\item{GridTune}{Set to TRUE if you want to grid tune the models}

\item{grid_eval_metric}{Select the metric to optimize in grid tuning. "accuracy", "microauc", "logloss"}

\item{MaxModelsInGrid}{Set to a numeric value for the number of models to try in grid tune}

\item{BaselineComparison}{"default"}

\item{MaxRunsWithoutNewWinner}{Number of runs without a new winner before stopping the grid tuning}

\item{MaxRunMinutes}{Max number of minutes to allow the grid tuning to run for}

\item{input_model}{= NULL, # continue training a model that is stored to fil}

\item{task}{'train' or 'refit'}

\item{device_type}{'cpu' or 'gpu'}

\item{objective}{'binary'}

\item{metric}{'binary_logloss', 'average_precision', 'auc', 'map', 'binary_error', 'auc_mu'}

\item{boosting}{'gbdt', 'rf', 'dart', 'goss'}

\item{LinearTree}{FALSE}

\item{Trees}{50L}

\item{eta}{NULL}

\item{num_leaves}{31}

\item{deterministic}{TRUE

# Learning Parameters https://lightgbm.readthedocs.io/en/latest/Parameters.html#learning-control-parameter}

\item{force_col_wise}{FALSE}

\item{force_row_wise}{FALSE}

\item{max_depth}{NULL}

\item{min_data_in_leaf}{20}

\item{min_sum_hessian_in_leaf}{0.001}

\item{bagging_freq}{0}

\item{bagging_fraction}{1.0}

\item{feature_fraction}{1.0}

\item{feature_fraction_bynode}{1.0}

\item{extra_trees}{FALSE}

\item{early_stopping_round}{10}

\item{first_metric_only}{TRUE}

\item{max_delta_step}{0.0}

\item{lambda_l1}{0.0}

\item{lambda_l2}{0.0}

\item{linear_lambda}{0.0}

\item{min_gain_to_split}{0}

\item{drop_rate_dart}{0.10}

\item{max_drop_dart}{50}

\item{skip_drop_dart}{0.50}

\item{uniform_drop_dart}{FALSE}

\item{top_rate_goss}{FALSE}

\item{other_rate_goss}{FALSE}

\item{monotone_constraints}{"gbdt_prediction.cpp"}

\item{monotone_constraints_method}{'advanced'}

\item{monotone_penalty}{0.0}

\item{forcedsplits_filename}{NULL # use for AutoStack option; .json fil}

\item{refit_decay_rate}{0.90}

\item{path_smooth}{0.0

# IO Dataset Parameters https://lightgbm.readthedocs.io/en/latest/Parameters.html#io-parameters}

\item{max_bin}{255}

\item{min_data_in_bin}{3}

\item{data_random_seed}{1}

\item{is_enable_sparse}{TRUE}

\item{enable_bundle}{TRUE}

\item{use_missing}{TRUE}

\item{zero_as_missing}{FALSE}

\item{two_round}{FALSE

# Convert Parameters # https://lightgbm.readthedocs.io/en/latest/Parameters.html#convert-parameters}

\item{convert_model}{'gbdt_prediction.cpp'}

\item{convert_model_language}{'cpp'

# Objective Parameters https://lightgbm.readthedocs.io/en/latest/Parameters.html#objective-parameters}

\item{boost_from_average}{TRUE}

\item{is_unbalance}{FALSE}

\item{scale_pos_weight}{1.0

# Metric Parameters (metric is in Core)}

\item{is_provide_training_metric}{TRUE}

\item{eval_at}{c(1,2,3,4,5)

# Network Parameter}

\item{num_machines}{1

# GPU Parameter}

\item{gpu_platform_id}{-1}

\item{gpu_device_id}{-1}

\item{gpu_use_dp}{TRUE}

\item{num_gpu}{1}
}
\description{
AutoLightGBMHurdleModel is generalized hurdle modeling framework
}
\examples{
\dontrun{
# Test data.table
LightGBM_QA <- data.table::CJ(
  TOF = c(TRUE,FALSE),
  Classification = c(TRUE,FALSE),
  Success = "Failure",
  ScoreSuccess = "Failure",
  PartitionInFunction = c(TRUE,FALSE), sorted = FALSE
)

# Remove impossible combinations
LightGBM_QA <- LightGBM_QA[!(PartitionInFunction & TOF)]
LightGBM_QA[, RunNumber := seq_len(.N)]

# Path File
Path <- getwd()

#      TOF Classification Success PartitionInFunction RunNumber
# 1:  TRUE           TRUE Failure               FALSE         1
# 2:  TRUE          FALSE Failure               FALSE         2
# 3: FALSE           TRUE Failure                TRUE         3
# 4: FALSE           TRUE Failure               FALSE         4
# 5: FALSE          FALSE Failure                TRUE         5
# 6: FALSE          FALSE Failure               FALSE         6

# AutoCatBoostHurdleModel
# run = 1
# run = 6
for(run in seq_len(LightGBM_QA[,.N])) {

  # Define values
  tof <- LightGBM_QA[run, TOF]
  PartitionInFunction <- LightGBM_QA[run, PartitionInFunction]
  Classify <- LightGBM_QA[run, Classification]
  Tar <- "Adrian"

  # Get data
  if(Classify) {
    data <- RemixAutoML::FakeDataGenerator(N = 15000, ZIP = 1)
  } else {
    data <- RemixAutoML::FakeDataGenerator(N = 100000, ZIP = 2)
  }

  # Partition Data
  if(!tof && !PartitionInFunction) {
    Sets <- RemixAutoML::AutoDataPartition(
      data = data,
      NumDataSets = 3,
      Ratios = c(0.7,0.2,0.1),
      PartitionType = "random",
      StratifyColumnNames = "Adrian",
      TimeColumnName = NULL)
    TTrainData <- Sets$TrainData
    VValidationData <- Sets$ValidationData
    TTestData <- Sets$TestData
    rm(Sets)
  } else {
    TTrainData <- data.table::copy(data)
    VValidationData <- NULL
    TTestData <- NULL
  }

  # Run function
  TestModel <- tryCatch({RemixAutoML::AutoLightGBMHurdleModel(

    # Operationalization
    ModelID = 'ModelTest',
    SaveModelObjects = FALSE,
    ReturnModelObjects = TRUE,
    NThreads = parallel::detectCores(),

    # Data related args
    data = TTrainData,
    ValidationData = VValidationData,
    PrimaryDateColumn = "DateTime",
    TestData = TTestData,
    WeightsColumnName = NULL,
    TrainOnFull = tof,
    Buckets = if(Classify) 0L else c(0,2,3),
    TargetColumnName = "Adrian",
    FeatureColNames = names(TTrainData)[!names(data) \%in\% c("Adrian","IDcol_1","IDcol_2","IDcol_3","IDcol_4","IDcol_5","DateTime")],
    IDcols = c("IDcol_1","IDcol_2","IDcol_3","IDcol_4","IDcol_5","DateTime"),
    DebugMode = TRUE,

    # Metadata args
    EncodingMethod = "credibility",
    Paths = getwd(),
    MetaDataPaths = NULL,
    TransformNumericColumns = NULL,
    Methods = c('Asinh', 'Asin', 'Log', 'LogPlus1', 'Logit'),
    ClassWeights = c(1,1),
    SplitRatios = if(PartitionInFunction) c(0.70, 0.20, 0.10) else NULL,
    NumOfParDepPlots = 10L,

    # Grid tuning setup
    PassInGrid = NULL,
    GridTune = FALSE,
    BaselineComparison = 'default',
    MaxModelsInGrid = 1L,
    MaxRunsWithoutNewWinner = 20L,
    MaxRunMinutes = 60L*60L,

    # LightGBM parameters
    task = list('classifier' = 'train', 'regression' = 'train'),
    device_type = list('classifier' = 'CPU', 'regression' = 'CPU'),
    objective = if(Classify) list('classifier' = 'binary', 'regression' = 'regression') else list('classifier' = 'multiclass', 'regression' = 'regression'),
    metric = if(Classify

    ) list('classifier' = 'binary_logloss', 'regression' = 'rmse') else list('classifier' = 'multi_logloss', 'regression' = 'rmse'),
    boosting = list('classifier' = 'gbdt', 'regression' = 'gbdt'),
    LinearTree = list('classifier' = FALSE, 'regression' = FALSE),
    Trees = list('classifier' = 50L, 'regression' = 50L),
    eta = list('classifier' = NULL, 'regression' = NULL),
    num_leaves = list('classifier' = 31, 'regression' = 31),
    deterministic = list('classifier' = TRUE, 'regression' = TRUE),

    # Learning Parameters
    force_col_wise = list('classifier' = FALSE, 'regression' = FALSE),
    force_row_wise = list('classifier' = FALSE, 'regression' = FALSE),
    max_depth = list('classifier' = NULL, 'regression' = NULL),
    min_data_in_leaf = list('classifier' = 20, 'regression' = 20),
    min_sum_hessian_in_leaf = list('classifier' = 0.001, 'regression' = 0.001),
    bagging_freq = list('classifier' = 0, 'regression' = 0),
    bagging_fraction = list('classifier' = 1.0, 'regression' = 1.0),
    feature_fraction = list('classifier' = 1.0, 'regression' = 1.0),
    feature_fraction_bynode = list('classifier' = 1.0, 'regression' = 1.0),
    extra_trees = list('classifier' = FALSE, 'regression' = FALSE),
    early_stopping_round = list('classifier' = 10, 'regression' = 10),
    first_metric_only = list('classifier' = TRUE, 'regression' = TRUE),
    max_delta_step = list('classifier' = 0.0, 'regression' = 0.0),
    lambda_l1 = list('classifier' = 0.0, 'regression' = 0.0),
    lambda_l2 = list('classifier' = 0.0, 'regression' = 0.0),
    linear_lambda = list('classifier' = 0.0, 'regression' = 0.0),
    min_gain_to_split = list('classifier' = 0, 'regression' = 0),
    drop_rate_dart = list('classifier' = 0.10, 'regression' = 0.10),
    max_drop_dart = list('classifier' = 50, 'regression' = 50),
    skip_drop_dart = list('classifier' = 0.50, 'regression' = 0.50),
    uniform_drop_dart = list('classifier' = FALSE, 'regression' = FALSE),
    top_rate_goss = list('classifier' = FALSE, 'regression' = FALSE),
    other_rate_goss = list('classifier' = FALSE, 'regression' = FALSE),
    monotone_constraints = list('classifier' = NULL, 'regression' = NULL),
    monotone_constraints_method = list('classifier' = 'advanced', 'regression' = 'advanced'),
    monotone_penalty = list('classifier' = 0.0, 'regression' = 0.0),
    forcedsplits_filename = list('classifier' = NULL, 'regression' = NULL),
    refit_decay_rate = list('classifier' = 0.90, 'regression' = 0.90),
    path_smooth = list('classifier' = 0.0, 'regression' = 0.0),

    # IO Dataset Parameters
    max_bin = list('classifier' = 255, 'regression' = 255),
    min_data_in_bin = list('classifier' = 3, 'regression' = 3),
    data_random_seed = list('classifier' = 1, 'regression' = 1),
    is_enable_sparse = list('classifier' = TRUE, 'regression' = TRUE),
    enable_bundle = list('classifier' = TRUE, 'regression' = TRUE),
    use_missing = list('classifier' = TRUE, 'regression' = TRUE),
    zero_as_missing = list('classifier' = FALSE, 'regression' = FALSE),
    two_round = list('classifier' = FALSE, 'regression' = FALSE),

    # Convert Parameters
    convert_model = list('classifier' = NULL, 'regression' = NULL),
    convert_model_language = list('classifier' = "cpp", 'regression' = "cpp"),

    # Objective Parameters
    boost_from_average = list('classifier' = TRUE, 'regression' = TRUE),
    is_unbalance = list('classifier' = FALSE, 'regression' = FALSE),
    scale_pos_weight = list('classifier' = 1.0, 'regression' = 1.0),

    # Metric Parameters (metric is in Core)
    is_provide_training_metric = list('classifier' = TRUE, 'regression' = TRUE),
    eval_at = list('classifier' = c(1,2,3,4,5), 'regression' = c(1,2,3,4,5)),

    # Network Parameters
    num_machines = list('classifier' = 1, 'regression' = 1),

    # GPU Parameters
    gpu_platform_id = list('classifier' = -1, 'regression' = -1),
    gpu_device_id = list('classifier' = -1, 'regression' = -1),
    gpu_use_dp = list('classifier' = TRUE, 'regression' = TRUE),
    num_gpu = list('classifier' = 1, 'regression' = 1))}, error = function(x) NULL)

  # Outcome
  if(!is.null(TestModel)) LightGBM_QA[run, Success := "Success"]
  data.table::fwrite(LightGBM_QA, file = "C:/Users/Bizon/Documents/GitHub/QA_Code/QA_CSV/AutoLightGBMHurdleModel_QA.csv")

  # Remove Target Variable
  TTrainData[, c("Target_Buckets", "Adrian") := NULL]

  # Score CatBoost Hurdle Model
  Output <- tryCatch({RemixAutoML::AutoLightGBMHurdleModelScoring(
    TestData = TTrainData,
    Path = Path,
    ModelID = "ModelTest",
    ModelList = TestModel$ModelList,
    ArgsList = TestModel$ArgsList,
    Threshold = NULL)}, error = function(x) NULL)

  # Outcome
  if(!is.null(Output)) LightGBM_QA[run, Score := "Success"]
  TestModel <- NULL
  Output <- NULL
  TTrainData <- NULL
  VValidationData <- NULL
  TTestData <- NULL
  gc(); Sys.sleep(5)
  data.table::fwrite(LightGBM_QA, file = file.path(Path, "AutoLightGBMHurdleModel_QA.csv"))
}
}
}
\seealso{
Other Supervised Learning - Hurdle Modeling: 
\code{\link{AutoCatBoostHurdleModel}()},
\code{\link{AutoH2oDRFHurdleModel}()},
\code{\link{AutoH2oGBMHurdleModel}()},
\code{\link{AutoXGBoostHurdleModel}()}
}
\author{
Adrian Antico
}
\concept{Supervised Learning - Hurdle Modeling}
